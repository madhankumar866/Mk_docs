{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>Welcome to MK Docs use it for Notification</p> <p>Madan Kumar</p>"},{"location":"#document-title","title":"Document Title","text":"<p>MK Documentation</p> <p>For full documentation visit domain.com</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":""},{"location":"about/","title":"ABOUT","text":"<p>Sorry Dude Thats's Classified</p>"},{"location":"AWS/Amazon-Kinesis/","title":"Amazon Kinesis","text":"<p>Read Office document for  amazon==&gt; Choose Document</p> <pre><code>https://aws.amazon.com/kinesis/getting-started/?nc=sn&amp;loc=3\n</code></pre> <p>sdk for in  github</p> <p>S3 bucket prefix :-</p> <p>Note</p> <p>shyaway-WAF-CDN-Global-logs/Access_logs/!{timestamp:yyyy}/!{timestamp:MM}/!{timestamp:dd}/!{timestamp:HH}/</p> <p>S3 bucket error prefix :-</p> <p>shyaway-WAF-CDN-Global-logs/Error-logs/year=!{firehose:error-output-type}/!{timestamp:yyyy'-'MM'-'dd}/\"</p> <p>Note</p> <p>Amazon Kinesis Data Firehose custom prefixes for Amazon S3 aws.com.</p> <p>aws.com.</p> <p>aws.com.</p> <p>aws.com.</p>"},{"location":"AWS/Amazon-Kinesis/#important","title":"Important","text":"<p>Dynamic-partition Data from Data to s3</p> <p>Streaming Data Solution for Amazon Kinesis</p>"},{"location":"AWS/Architectures/","title":"Refernce_links","text":"<p>(ecs batch processing)[https://github.com/aws-samples/ecs-refarch-batch-processing]</p>"},{"location":"AWS/Aws-Athena/","title":"Amazon  Athena","text":""},{"location":"AWS/Aws-Athena/#_1","title":"Aws athena","text":"<p>To count number of request url path per ip's SELECT DISTINCT client_ip,         request_url,          count() AS count FROM alb_logs WHERE parse_datetime(time,'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z')     BETWEEN parse_datetime('2021-09-08-00:00:00','yyyy-MM-dd-HH:mm:ss')         AND parse_datetime('2021-09-08-23:59:00','yyyy-MM-dd-HH:mm:ss') GROUP BY  client_ip,request_url ORDER BY  count() DESC</p>"},{"location":"AWS/Aws-Athena/#_2","title":"Aws athena","text":"<p>WITH dataset as ( SELECT action as waf_action, terminatingRuleType waf_rule_type, terminatingruleid waf_rule_id,timestamp, httprequest.clientip, httprequest.country, headeritems AS header, httprequest.uri, httprequest.args FROM \"shyaway_logs\".\"waf_global_alb_logs\" waf CROSS JOIN UNNEST(httprequest.headers) AS t(headeritems) ) select count(*),waf_action, waf_rule_type,waf_rule_id, clientip, country, header.value, uri, args from dataset WHERE waf_action='BLOCK' AND timestamp between 1606483800000 and 1606548600000 GROUP BY waf_action,waf_rule_id,waf_rule_type,clientip,country,header.value, uri,args</p>"},{"location":"AWS/Aws-Athena/#_3","title":"Aws athena","text":"<p>WITH dataset as ( SELECT action as waf_action, terminatingRuleType waf_rule_type, terminatingruleid test, httprequest.clientip, httprequest.country, headeritems AS header, httprequest.uri, httprequest.args FROM \"shyaway_logs\".\"waf_global_alb_logs\" waf CROSS JOIN UNNEST(httprequest.headers) AS t(headeritems) ) select waf_action, waf_rule_type,test, clientip, country, header.value, uri, args from dataset where header.name='user-agent'  and header.value='facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)' and waf_action='BLOCK' limit 10</p>"},{"location":"AWS/Aws-Athena/#_4","title":"Aws athena","text":"<p>SELECT COUNT(*) AS count,httpRequest.country, terminatingruleid, httprequest.clientip, action, httprequest.uri,timestamp FROM shyaway_waf_cdn_global_logs WHERE action='BLOCK' GROUP BY timestamp,httpRequest.country,terminatingruleid, httprequest.clientip, httprequest.uri, action ORDER BY count DESC,timestamp DESC LIMIT 1000;</p>"},{"location":"AWS/Aws-Athena/#_5","title":"Aws athena","text":"<p>SELECT * FROM alb_logs WHERE (\"request_url\" = 'https://www.shyaway.com:443/bra-online/?bra_offers=buy-2-get-3-free&amp;sku=S28035-Red&amp;utm_source=fblk1p&amp;utm_medium=bra&amp;utm_campaign=999off') limit 10</p>"},{"location":"AWS/Aws-Athena/#_6","title":"Aws athena","text":""},{"location":"AWS/Aws-Athena/#_7","title":"Aws athena","text":"<p>CREATE OR REPLACE VIEW count-view-alb-logs AS  SELECT   \"elb\" , \"count\"(*) \"count\" FROM   alb_logs WHERE (\"parse_datetime\"(\"time\", 'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z') BETWEEN \"parse_datetime\"('2020-10-11-00:00:00', 'yyyy-MM-dd-HH:mm:ss') AND \"parse_datetime\"('2020-10-11-03:00:00', 'yyyy-MM-dd-HH:mm:ss')) GROUP BY \"elb\" LIMIT 100</p>"},{"location":"AWS/Aws-Athena/#_8","title":"Aws athena","text":"<p>SELECT elb,count(*) FROM \"alb_logs\" WHERE parse_datetime(time,'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z')     BETWEEN parse_datetime('2020-11-26-00:00:00','yyyy-MM-dd-HH:mm:ss')         AND parse_datetime('2020-11-27-23:59:00','yyyy-MM-dd-HH:mm:ss') GROUP BY elb limit 10</p>"},{"location":"AWS/Aws-Athena/#_9","title":"Aws athena","text":"<p>SELECT COUNT(*) AS count,elb_status_code,request_url,user_agent FROM alb_logs WHERE request_url='https://www.shyaway.com:443/bra-online/?bra_offers=buy-2-get-3-free&amp;sku=S28035-Red&amp;utm_source=fblk1p&amp;utm_medium=bra&amp;utm_campaign=999off' GROUP BY elb_status_code,request_url,user_agent ORDER BY count DESC LIMIT 10;</p>"},{"location":"AWS/Aws-Athena/#_10","title":"Aws athena","text":"<p>SELECT COUNT(*) AS count,elb_status_code,request_url,user_agent FROM alb_logs WHERE user_agent='facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)' AND elb_status_code='403' GROUP BY request_url,elb_status_code,user_agent ORDER BY count DESC LIMIT 10;</p>"},{"location":"AWS/Aws-Athena/#_11","title":"Aws athena","text":"<p>SELECT COUNT(*) AS count,httpRequest.country, terminatingruleid, httprequest.clientip, action, httprequest.uri,timestamp FROM waf_global_alb_logs WHERE action='BLOCK' AND timestamp between 1606503600000 and 1606863600000 GROUP BY timestamp,httpRequest.country,terminatingruleid, httprequest.clientip, httprequest.uri, action ORDER BY count DESC,timestamp DESC limit 10;</p>"},{"location":"AWS/Aws-Athena/#_12","title":"Aws athena","text":"<p>WITH dataset as ( SELECT action as waf_action, terminatingRuleType waf_rule_type, terminatingruleid waf_rule_id,timestamp, httprequest.clientip, httprequest.country, headeritems AS header, httprequest.uri, httprequest.args FROM \"shyaway_logs\".\"waf_global_alb_logs\" waf CROSS JOIN UNNEST(httprequest.headers) AS t(headeritems) ) select count(*),waf_action, waf_rule_type,waf_rule_id, clientip, country, header.value, uri, args,timestamp,uri from dataset WHERE waf_action='BLOCK' AND timestamp between 1606503600000 and 1606863600000 GROUP BY waf_action,timestamp,waf_rule_id,waf_rule_type,clientip,country,header.value, uri,args,timestamp,uri</p>"},{"location":"AWS/Aws-Athena/#_13","title":"Aws athena","text":"<p>SELECT  elb_status_code,client_ip,request_url,user_agent,          count(*) AS count FROM \"shyaway_logs\".\"alb_log\" WHERE   elb_status_code LIKE '%400%' AND parse_datetime(time,'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z')     BETWEEN parse_datetime('2021-01-04-01:01:00','yyyy-MM-dd-HH:mm:ss')         AND parse_datetime('2021-01-04-23:59:00','yyyy-MM-dd-HH:mm:ss') GROUP BY    elb_status_code,client_ip,request_url,user_agent ORDER BY  count DESC,request_url DESC,user_agent DESC</p>"},{"location":"AWS/Aws-Athena/#_14","title":"Aws athena","text":"<p>WITH dataset as ( SELECT action as waf_action, terminatingRuleType waf_rule_type, terminatingruleid waf_rule_id,timestamp, httprequest.clientip, httprequest.country, headeritems AS header, httprequest.uri, httprequest.args FROM \"shyaway_logs\".\"waf_global_alb_logs\" waf CROSS JOIN UNNEST(httprequest.headers) AS t(headeritems) ) select count(*),waf_action, waf_rule_type,waf_rule_id, clientip, country, header.value, uri, args,timestamp,uri from dataset WHERE waf_action='BLOCK' AND timestamp &gt; 1608187410000 GROUP BY waf_action,timestamp,wafj_rule_id,waf_rule_type,clientip,country,header.value, uri,args,uri</p>"},{"location":"AWS/Aws-Athena/#_15","title":"Aws athena","text":""},{"location":"AWS/Aws-well-architech/","title":"wellarchitectedlabs","text":"<p>https://www.wellarchitectedlabs.com/</p>"},{"location":"AWS/CI-CD/","title":"Ci cd","text":""},{"location":"AWS/CI-CD/#by-twitch","title":"by twitch","text":"<p>The Big Dev Theory cicd sample coode https://github.com/cycode-aws-demo/demo</p>"},{"location":"AWS/DMS/","title":"Database Migration System AWS","text":"<p>For DMS TASK    </p> <p>Provide the Following Aws customer Details To the Magento or Service provider</p> <p>Aws account number Region Vpc Subnet Sepcify the port number to be opened by the service provider</p> <p>Get the \"Endpoint\" from Adobe or whoever is providing the service &amp; Look out for \"Endpoint\" under Vpc in aws</p> <p>Paste the Provided \"Endpoint\" in \"endpoint\" in vpc &amp; choose \"Find service by name\" and paste the endpoint  in same vpc</p> <p>After Creation use the \"DNS names\" in endpoint  to connect to appropriate service and port number</p> <p>Example:- curl -v telnet://\"DNS names\":80 -vvv </p> <p>curl -v telnet://vpce-007ffnb9qkcnjgult-yfhmywqh.vpce-svc-083cqvm2ta3rxqat5v.us-east-1.vpce.amazonaws.com:80 -vvv </p> <p>Create the service or instance in same region,vpc,&amp; subnet's</p> <p>Permission should be done In Master Db</p> <p>GRANT REPLICATION CLIENT, REPLICATION SLAVE ON Db.* TO 'passwd'@'%';</p> <p>It should be done by the provider or the root in master</p> <p>Permission should be done In SLAVE DB</p> <p>GRANT ALTER, CREATE, DROP, INDEX, INSERT, UPDATE, DELETE, SELECT ON . TO'passwd'@'%'; GRANT ALL PRIVILEGES ON awsdms_control.* TO ''@'%'; </p> <p>CREATE USER 'username'@'%' IDENTIFIED BY 'passwd';</p> <p>GRANT ALL PRIVILEGES ON * . * TO 'passwd'@'%';</p> <p>FLUSH PRIVILEGES;</p> <p>And create the target endpoint for master and slave and test the connection.</p>"},{"location":"AWS/Databases/","title":"Databases on AWS: The Right Tool for the Right Job","text":"<p>DB</p>"},{"location":"AWS/Dynamodb/","title":"dynamo Db","text":""},{"location":"AWS/Dynamodb/#dynamodb-session-handler","title":"DynamoDB Session Handler","text":"<p>https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html</p>"},{"location":"AWS/EBS/","title":"Ebs","text":"<p>https://ahmedahamid.com/which-is-better/#:~:text=The%20graph%20shows%20that%20gp3,allows%20a%20much%20higher%20throughput.</p>"},{"location":"AWS/EC2/","title":"Ec2","text":"<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"AWS/EC2/#ec2-ssh-error","title":"ec2 ssh error","text":""},{"location":"AWS/EC2/#ec2-ssh-errorhttpsawsamazoncompremiumsupportknowledge-centerec2-linux-resolve-ssh-connection-errors","title":"(ec2 ssh error)[https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/]","text":""},{"location":"AWS/EC2/#ec2-ssh-error_1","title":"ec2 ssh error","text":"<p>(ec2 ssh error)[https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/]</p> <p>1a23421 (Rebase_Edited_Update)</p>"},{"location":"AWS/IAM/","title":"Iam","text":""},{"location":"AWS/IAM/#code-commit-policy","title":"code commit policy","text":"<p>Allow only Specific user to access branch</p> <p>AWS CodeCommit</p> Policy <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"codecommit:*\",\n            \"Resource\": \"repoarn\",\n            \"Condition\": {\n                \"StringEqualsIfExists\": {\n                    \"codecommit:References\": [\n                        \"refs/heads/Branch-name\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li><code>Git command to clone</code></li> </ul> <p>git clone --single-branch -b Branch-name repo-url</p> <p>https://medium.com/@it.melnichenko/invoke-a-lambda-across-multiple-aws-accounts-8c094b2e70be</p> Limiting restrict user from accessing repo <pre><code>\n</code></pre>"},{"location":"AWS/Nat_instance/","title":"Nat_instance","text":""},{"location":"AWS/Nat_instance/#nat-instance-setup-guidence","title":"NAT instance setup Guidence","text":"<p>Nat_Setup</p> <p>Points to note</p> <pre><code>* Allow Each subnetentries in both security group in both eks and ec2 instance both security group to allow  communication\n* Note Ethernet eni interface number when command execution \"eth0\"it may changed using aws images.\n\n\n----\n</code></pre>"},{"location":"AWS/Nat_instance/#enable-private-resources-to-communicate-outside-the-vpc-amazon-virtual-private-cloud","title":"Enable private resources to communicate outside the VPC - Amazon Virtual Private Cloud","text":"<p>This section describes how to create and work with NAT instances to enable resources in a private subnet to communicate outside the virtual private cloud.</p>"},{"location":"AWS/Nat_instance/#tasks","title":"Tasks","text":"<ul> <li>1. Create a VPC for the NAT instance</li> <li>2. Create a security group for the NAT instance</li> <li>3. Create a NAT AMI</li> <li>4. Launch a NAT instance</li> <li>5. Disable source/destination checks</li> <li>6. Update the route table</li> <li>7. Test your NAT instance</li> </ul>"},{"location":"AWS/Nat_instance/#1-create-a-vpc-for-the-nat-instance","title":"1. Create a VPC for the NAT instance","text":"<p>Use the following procedure to create a VPC with a public subnet and a private subnet.</p>"},{"location":"AWS/Nat_instance/#to-create-the-vpc","title":"To create the VPC","text":"<ol> <li> <p>Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.</p> </li> <li> <p>Choose Create VPC.</p> </li> <li> <p>For Resources to create, choose VPC and more.</p> </li> <li> <p>For Name tag auto-generation, enter a name for the VPC.</p> </li> <li> <p>To configure the subnets, do the following:</p> <ol> <li> <p>For Number of Availability Zones, choose 1 or 2, depending on your needs.</p> </li> <li> <p>For Number of public subnets, ensure that you have one public subnet per Availability Zone.</p> </li> <li> <p>For Number of private subnets, ensure that you have one private subnet per Availability Zone.</p> </li> </ol> </li> <li> <p>Choose Create VPC.</p> </li> </ol>"},{"location":"AWS/Nat_instance/#2-create-a-security-group-for-the-nat-instance","title":"2. Create a security group for the NAT instance","text":"<p>Create a security group with the rules described in the following table. These rules enable your NAT instance to receive internet-bound traffic from instances in the private subnet, as well as SSH traffic from your network. The NAT instance can also send traffic to the internet, which enables the instances in the private subnet to get software updates.</p> <p>The following are the inbound recommended rules.</p> Source Protocol Port range Comments Private subnet CIDR TCP 80 Allow inbound HTTP traffic from servers in the private subnetinternet Private subnet CIDR TCP 443 Allow inbound HTTPS traffic from servers in the private subnetinternet Public IP address range of your network TCP 22 Allow inbound SSH access to the NAT instance from your network (over the internet gateway) <p>The following are the recommended outbound rules.</p> Destination Protocol Port range Comments 0.0.0.0/0 TCP 80 Allow outbound HTTP access to the internet 0.0.0.0/0 TCP 443 Allow outbound HTTPS access to the internet"},{"location":"AWS/Nat_instance/#to-create-the-security-group","title":"To create the security group","text":"<ol> <li> <p>Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.</p> </li> <li> <p>In the navigation pane, choose Security groups.</p> </li> <li> <p>Choose Create security group.</p> </li> <li> <p>Enter a name and description for the security group.</p> </li> <li> <p>For VPC, select the ID of the VPC for your NAT instance.</p> </li> <li> <p>Add rules for inbound traffic under Inbound rules as follows:</p> <ol> <li> <p>Choose Add rule. Choose HTTP for Type and enter the IP address range of your private subnet for Source.</p> </li> <li> <p>Choose Add rule. Choose HTTPS for Type and enter the IP address range of your private subnet for Source.</p> </li> <li> <p>Choose Add rule. Choose SSH for Type and enter the IP address range of your network for Source.</p> </li> </ol> </li> <li> <p>Add rules for outbound traffic under Outbound rules as follows:</p> <ol> <li> <p>Choose Add rule. Choose HTTP for Type and enter 0.0.0.0/0 for Destination.</p> </li> <li> <p>Choose Add rule. Choose HTTPS for Type and enter 0.0.0.0/0 for Destination.</p> </li> </ol> </li> <li> <p>Choose Create security group.</p> </li> </ol> <p>For more information, see Security groups.</p>"},{"location":"AWS/Nat_instance/#3-create-a-nat-ami","title":"3. Create a NAT AMI","text":"<p>A NAT AMI is configured to run NAT on an EC2 instance. You must create a NAT AMI and then launch your NAT instance using your NAT AMI.</p> <p>If you plan to use an operating system other than Amazon Linux for your NAT AMI, refer to the documentation for this operating system to learn how to configure NAT. Be sure to save these settings so that they persist even after an instance reboot.</p>"},{"location":"AWS/Nat_instance/#to-create-a-nat-ami-for-amazon-linux","title":"To create a NAT AMI for Amazon Linux","text":"<ol> <li> <p>Launch an EC2 instance running AL2023 or Amazon Linux 2. Be sure to specify the security group that you created for the NAT instance.</p> </li> <li> <p>Connect to your instance and run the following commands on the instance to enable iptables.</p> </li> </ol> <pre><code>sudo yum install iptables-services -y\nsudo systemctl enable iptables\nsudo systemctl start iptables\n</code></pre> <ol> <li> <p>Do the following on the instance to enable IP forwarding such that it persists after reboot:</p> <ol> <li> <p>Using a text editor, such as nano or vim, create the following configuration file: <code>/etc/sysctl.d/custom-ip-forwarding.conf</code>.</p> </li> <li> <p>Add the following line to the configuration file.</p> </li> </ol> </li> </ol> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <pre><code>3.  Save the configuration file and exit the text editor.\n\n4.  Run the following command to apply the configuration file.\n</code></pre> <pre><code>sudo sysctl -p /etc/sysctl.d/custom-ip-forwarding.conf\n</code></pre> <ol> <li>Run the following command on the instance, and note the name of the primary network interface. You'll need this information for the next step.</li> </ol> <pre><code>netstat -i\n</code></pre> <p>In the following example output, <code>docker0</code> is a network interface created by docker, <code>eth0</code> is the primary network interface, and <code>lo</code> is the loopback interface.</p> <pre><code>Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg\ndocker0   1500        0      0      0 0             0      0      0      0 BMU\neth0      9001  7276052      0      0 0       5364991      0      0      0 BMRU\nlo       65536   538857      0      0 0        538857      0      0      0 LRU\n</code></pre> <p>In the following example output, the primary network interface is <code>enX0</code>.</p> <pre><code>Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg\nenX0      9001     1076      0      0 0          1247      0      0      0 BMRU\nlo       65536       24      0      0 0            24      0      0      0 LRU\n</code></pre> <p>In the following example output, the primary network interface is <code>ens5</code>.</p> <pre><code>Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg\nens5      9001    14036      0      0 0          2116      0      0      0 BMRU\nlo       65536       12      0      0 0            12      0      0      0 LRU\n</code></pre> <ol> <li>Run the following commands on the instance to configure NAT. If the primary network interface is not <code>eth0</code>, replace <code>eth0</code> with the primary network interface that you noted in the previous step.</li> </ol> <pre><code>sudo /sbin/iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\nsudo /sbin/iptables -F FORWARD\nsudo service iptables save\n</code></pre> <ol> <li>Create a NAT AMI from the EC2 instance. For more information, see Create a Linux AMI from an instance in the Amazon EC2 User Guide.</li> </ol>"},{"location":"AWS/Nat_instance/#4-launch-a-nat-instance","title":"4. Launch a NAT instance","text":"<p>Use the following procedure to launch a NAT instance using the VPC, security group, and NAT AMI that you created.</p>"},{"location":"AWS/Nat_instance/#to-launch-a-nat-instance","title":"To launch a NAT instance","text":"<ol> <li> <p>Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.</p> </li> <li> <p>On the dashboard, choose Launch instance.</p> </li> <li> <p>For Name, enter a name for your NAT instance.</p> </li> <li> <p>For Application and OS Images, select your NAT AMI (choose Browse more AMIs, My AMIs).</p> </li> <li> <p>For Instance type, choose an instance type that provides the compute, memory, and storage resources that your NAT instance needs.</p> </li> <li> <p>For Key pair, select an existing key pair or choose Create new key pair.</p> </li> <li> <p>For Network settings, do the following:</p> <ol> <li> <p>Choose Edit.</p> </li> <li> <p>For VPC, choose the VPC that you created.</p> </li> <li> <p>For Subnet, choose the public subnet that you created.</p> </li> <li> <p>For Auto-assign public IP, choose Enable. Alternatively, after you launch the NAT instance, allocate an Elastic IP address and assign it to the NAT instance.</p> </li> <li> <p>For Firewall, choose Select existing security group and then choose the security group that you created.</p> </li> </ol> </li> <li> <p>Choose Launch instance. Choose the instance ID to open the instance details page. Wait for the instance state to change to Running and for the status checks to succeed.</p> </li> <li> <p>Disable source/destination checks for the NAT instance (see 5. Disable source/destination checks).</p> </li> <li> <p>Update the route table to send traffic to the NAT instance (see 6. Update the route table).</p> </li> </ol>"},{"location":"AWS/Nat_instance/#5-disable-sourcedestination-checks","title":"5. Disable source/destination checks","text":"<p>Each EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, a NAT instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on the NAT instance.</p>"},{"location":"AWS/Nat_instance/#to-disable-sourcedestination-checking","title":"To disable source/destination checking","text":"<ol> <li> <p>Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.</p> </li> <li> <p>In the navigation pane, choose Instances.</p> </li> <li> <p>Select the NAT instance.</p> </li> <li> <p>Choose Actions, Networking, Change source/destination check.</p> </li> <li> <p>For Source/destination checking, select Stop.</p> </li> <li> <p>Choose Save.</p> </li> <li> <p>If the NAT instance has a secondary network interface, choose it from Network interfaces on the Networking tab. Choose the interface ID to go to the network interfaces page. Choose Actions, Change source/dest. check, clear Enable, and choose Save.</p> </li> </ol>"},{"location":"AWS/Nat_instance/#6-update-the-route-table","title":"6. Update the route table","text":"<p>The route table for the private subnet must have a route that sends internet traffic to the NAT instance.</p>"},{"location":"AWS/Nat_instance/#to-update-the-route-table","title":"To update the route table","text":"<ol> <li> <p>Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.</p> </li> <li> <p>In the navigation pane, choose Route tables.</p> </li> <li> <p>Select the route table for the private subnet.</p> </li> <li> <p>On the Routes tab, choose Edit routes and then choose Add route.</p> </li> <li> <p>Enter 0.0.0.0/0 for Destination and the instance ID of the NAT instance for Target.</p> </li> <li> <p>Choose Save changes.</p> </li> </ol> <p>For more information, see Configure route tables.</p>"},{"location":"AWS/Nat_instance/#7-test-your-nat-instance","title":"7. Test your NAT instance","text":"<p>After you have launched a NAT instance and completed the configuration steps above, you can test whether an instance in your private subnet can access the internet through the NAT instance by using the NAT instance as a bastion server.</p>"},{"location":"AWS/Nat_instance/#tasks_1","title":"Tasks","text":"<ul> <li>Step 1: Update the NAT instance security group</li> <li>Step 2: Launch a test instance in the private subnet</li> <li>Step 3: Ping an ICMP-enabled website</li> <li>Step 4: Clean up</li> </ul>"},{"location":"AWS/Nat_instance/#step-1-update-the-nat-instance-security-group","title":"Step 1: Update the NAT instance security group","text":"<p>To allow instances in your private subnet to send ping traffic to the NAT instance, add a rule to allow inbound and outbound ICMP traffic. To allow the NAT instance to serve as a bastion server, add a rule to allow outbound SSH traffic to the private subnet.</p>"},{"location":"AWS/Nat_instance/#to-update-your-nat-instance-security-group","title":"To update your NAT instance security group","text":"<ol> <li> <p>Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.</p> </li> <li> <p>In the navigation pane, choose Security groups.</p> </li> <li> <p>Select the check box for the security group associated with your NAT instance.</p> </li> <li> <p>On the Inbound rules tab, choose Edit inbound rules.</p> </li> <li> <p>Choose Add rule. Choose All ICMP - IPv4 for Type. Choose Custom for Source and enter the IP address range of your private subnet. Choose Save rules.</p> </li> <li> <p>On the Outbound rules tab, choose Edit outbound rules.</p> </li> <li> <p>Choose Add rule. Choose SSH for Type. Choose Custom for Destination and enter the IP address range of your private subnet.</p> </li> <li> <p>Choose Add rule. Choose All ICMP - IPv4 for Type. Choose Anywhere - IPv4 for Destination. Choose Save rules.</p> </li> </ol>"},{"location":"AWS/Nat_instance/#step-2-launch-a-test-instance-in-the-private-subnet","title":"Step 2: Launch a test instance in the private subnet","text":"<p>Launch an instance into your private subnet. You must allow SSH access from the NAT instance, and you must use the same key pair that you used for the NAT instance.</p>"},{"location":"AWS/Nat_instance/#to-launch-a-test-instance-in-the-private-subnet","title":"To launch a test instance in the private subnet","text":"<ol> <li> <p>Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.</p> </li> <li> <p>On the dashboard, choose Launch instance.</p> </li> <li> <p>Select your private subnet.</p> </li> <li> <p>Do not assign a public IP address to this instance.</p> </li> <li> <p>Ensure that the security group for this instance allows inbound SSH access from your NAT instance, or from the IP address range of your public subnet, and outbound ICMP traffic.</p> </li> <li> <p>Select the same key pair that you used for the NAT instance.</p> </li> </ol>"},{"location":"AWS/Nat_instance/#step-3-ping-an-icmp-enabled-website","title":"Step 3: Ping an ICMP-enabled website","text":"<p>To verify that the test instance in your private subnet can use your NAT instance to communicate with the internet, run the ping command.</p>"},{"location":"AWS/Nat_instance/#to-test-the-internet-connection-from-your-private-instance","title":"To test the internet connection from your private instance","text":"<ol> <li> <p>From your local computer, configure SSH agent forwarding, so that you can use the NAT instance as a bastion server. <pre><code>ssh-add key.pem\n</code></pre></p> </li> <li> <p>From your local computer, connect to your NAT instance. <pre><code>ssh -A ec2-user@nat-instance-public-ip-address\n</code></pre></p> </li> <li> <p>From the NAT instance, run the ping command, specifying a website that is enabled for ICMP.</p> </li> </ol> <pre><code>[ec2-user@ip-10-0-4-184]$ ping ietf.org\n</code></pre> <p>To confirm that your NAT instance has internet access, verify that you received output such as the following, and then press Ctrl+C to cancel the ping command. Otherwise, verify that the NAT instance is in a public subnet (its route table has a route to an internet gateway).</p> <pre><code>PING ietf.org (104.16.45.99) 56(84) bytes of data.\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=1 ttl=33 time=7.88 ms\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=2 ttl=33 time=8.09 ms\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=3 ttl=33 time=7.97 ms\n...\n</code></pre> <ol> <li>From your NAT instance, connect to your instance in your private subnet by using its private IP address.</li> </ol> <pre><code>[ec2-user@ip-10-0-4-184]$ ssh ec2-user@private-server-private-ip-address\n</code></pre> <ol> <li>From your private instance, test that you can connect to the internet by running the ping command.</li> </ol> <pre><code>[ec2-user@ip-10-0-135-25]$ ping ietf.org\n</code></pre> <p>To confirm that your private instance has internet access through the NAT instance verify that you received output such as the following, and then press Ctrl+C to cancel the ping command.</p> <pre><code>PING ietf.org (104.16.45.99) 56(84) bytes of data.\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=1 ttl=33 time=8.76 ms\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=2 ttl=33 time=8.26 ms\n64 bytes from 104.16.45.99 (104.16.45.99): icmp_seq=3 ttl=33 time=8.27 ms\n...\n</code></pre>"},{"location":"AWS/Nat_instance/#troubleshooting","title":"Troubleshooting","text":"<p>If the ping command fails from the server in the private subnet, use the following steps to troubleshoot the issue:</p> <ul> <li> <p>Verify that you pinged a website that has ICMP enabled. Otherwise, your server can't receive reply packets. To test this, run the same ping command from a command line terminal on your own computer.</p> </li> <li> <p>Verify that the security group for your NAT instance allows inbound ICMP traffic from your private subnet. Otherwise, your NAT instance can't receive the ping command from your private instance.</p> </li> <li> <p>Verify that you disabled source/destination checking for your NAT instance. For more information, see 5. Disable source/destination checks.</p> </li> <li> <p>Verify that you configured your route tables correctly. For more information, see 6. Update the route table.</p> </li> </ul>"},{"location":"AWS/Nat_instance/#step-4-clean-up","title":"Step 4: Clean up","text":"<p>If you no longer require the test server in the private subnet, terminate the instance so that you are no longer billed for it. For more information, see Terminate your instance in the Amazon EC2 User Guide.</p> <p>If you no longer require the NAT instance, you can stop or terminate it, so that you are no longer billed for it. If you created a NAT AMI, you can create a new NAT instance whenever you need one.</p> <p>https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</p>"},{"location":"AWS/RDS/","title":"RDS","text":"<p>RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database and determine when and where to take action.</p>"},{"location":"AWS/Sqs/","title":"Sqs","text":""},{"location":"AWS/Sqs/#sendmessagebatch","title":"SendMessageBatch","text":"<p>You can use SendMessageBatch to send up to 10 messages to the specified queue link</p> <p>sdk for php</p> <p>link</p> <p>api gateway ==&gt; sqs ==&gt;  fluentd to instance  or lambda</p>"},{"location":"AWS/TLS-migration/","title":"Tls migration","text":"<p>TLS 1.2 to become the minimum TLS protocol level for all AWS API endpoints</p> <p>https://aws.amazon.com/blogs/security/tls-1-2-required-for-aws-endpoints/</p>"},{"location":"AWS/amazon-linux/","title":"AMAZON LINUX","text":"<p>use locate command to find the files  for starting \"\"service php56-php-fpm status\"\"</p>"},{"location":"AWS/amazon-linux/#install-php56-in-amazon-linux","title":"Install PHP5.6 in AMAZON LINUX","text":"<p>cat /etc/os-release &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm ======= yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</p> <p>ln -sf php56 php php -v</p>"},{"location":"AWS/amazon-linux/#1a23421-rebase_edited_update","title":"&gt;&gt;&gt;&gt;&gt;&gt; 1a23421 (Rebase_Edited_Update)","text":"<p>yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</p> <p>33a4921 (AWS) yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm -y yum install yum-utils -y yum-config-manager --enable remi-php56  yum install php56 php56-mcrypt php56-cli php56-gd php56-curl php56-mysql php56-ldap php56-zip php56-fileinfo php56-php-fpm install php56-php-pecl-mysqlnd-ms php56-php-pdo -y cd /usr/bin ln -sf php56 php</p>"},{"location":"AWS/amazon-linux/#install-mysql5555","title":"Install mysql5.5.55","text":"<p>wget https://cdn.mysql.com/archives/mysql-5.5/MySQL-5.5.55-1.el7.x86_64.rpm-bundle.tar tar -xvf MySQL-5.5. rm -rf .tar yum install * cat /etc/my.conf</p> <p>Check my.cnf and set correct access log PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER ! To do so, start the server, then issue the following commands:     /usr/bin/mysqladmin -u root password '*'     /usr/bin/mysqladmin -u root -h ip-172-31-92-146.ec2.internal password '***'</p> <pre><code>Alternatively you can run:\n/usr/bin/mysql_secure_installation\n</code></pre> <p>sudo service mysql stop sudo nano /etc/mysql/my.cnf</p> <p>add it in my.cof user=mysql</p> <p>sudo chown -R mysql:mysql /var/lib/mysql/ sudo service mysql start</p>"},{"location":"AWS/bot-fake-crawler-bots/","title":"Bot fake crawler bots","text":"<p>Bot crawler[aws.com]](https://aws.amazon.com/blogs/architecture/field-notes-how-to-identify-and-block-fake-crawler-bots-using-aws-waf/).</p>"},{"location":"AWS/codepipeline/","title":"code commit  pipeline","text":"<p>https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials.html</p> <p>https://pipelines.devops.aws.dev/</p>"},{"location":"AWS/exam-prep/","title":"Exam-prep","text":"<p>proceed by elimination  (rule out the answers that you know for sure are wrong) remaning answers, understand which one makes the  most sense if a solution seems feesable by highly complicated, probably it is wrong</p> <p>white papers :     archecting for the cloud: aws best practies     aws well-archited framewrok     aws disaster recovery</p> <p>read each services faq:     faq covers lot of questions asked in exams     example : h</p>"},{"location":"AWS/lambda/","title":"serverless testing","text":"<p>https://dev.to/mlabouardy/devops-bulletin-86-serverless-testing-61j</p> <p>https://dev.to/kumo/learn-serverless-on-aws-step-by-step-databases-kkg</p>"},{"location":"AWS/mysqld/","title":"Mysqld","text":"<p>[mysqld] port                    = 3306 datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock tmpdir                          = /var/lib/mysql/mysql_temp slave-skip-errors=1062,1146,1053,1064,1032,1677 slow_query_log          = ON long_query_time         = 10 slow_query_log_file     = /var/log/mysqld/slow.log</p> <p>read_buffer_size        = 8M  sort_buffer_size        = 8M </p>"},{"location":"AWS/mysqld/#disabling-symbolic-links-is-recommended-to-prevent-assorted-security-risks","title":"Disabling symbolic-links is recommended to prevent assorted security risks","text":"<p>symbolic-links=0 user=mysql</p>"},{"location":"AWS/mysqld/#innodb","title":"INNODB","text":"<p>innodb-buffer-pool-size        = 24M innodb-flush-method            = O_DIRECT innodb-flush-log-at-trx-commit = 2 innodb_buffer_pool_instances   = 8 innodb_log_file_size           = 50M innodb_log_buffer_size         = 32M innodb_thread_concurrency      = 8</p>"},{"location":"AWS/mysqld/#caches-and-limits","title":"CACHES AND LIMITS","text":""},{"location":"AWS/mysqld/#tmp-table-size-32m","title":"tmp-table-size                 = 32M","text":"<p>max-heap-table-size            = 32M query_cache_type               = 1 query_cache_size               = 800M max_connections                = 1700 wait_timeout                   = 800 thread_cache_size              = 512 open-files-limit               = 65535 table-definition-cache         = 1024 table-open-cache               = 2048</p>"},{"location":"AWS/mysqld/#tuning","title":"TUNING","text":"<p>max-allowed-packet             = 1G max-connect-errors             = 1000</p>"},{"location":"AWS/mysqld/#binlog","title":"Binlog","text":"<p>log-bin                        = /var/lib/mysql/mysql-bin expire-logs-days               = 14 sync-binlog                    = 1</p> <p>[mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid</p> <p>[client] port                    = 3306 socket                  = /var/lib/mysql/mysql.sock</p> <p>Tuining mysql </p> <p>read_buffer_size                = 62M sort_buffer_size                = 62M innodb_buffer_pool_size      = 24M read_buffer_size                = 62M --- 1MB for every 1GB of RAM) sort_buffer_size                = 62M --- 1MB for every 1GB of RAM) innodb_buffer_pool_size      = 24M</p>"},{"location":"AWS/remote-ec2/","title":"Remotely-run-commands-EC2-Instance-Systems-Manage","text":""},{"location":"AWS/remote-ec2/#ec2-credit-system","title":"EC2-Credit System","text":"<p>Understanding T2 Standard Inastance CPU Credits.</p> <p>Key concepts and definitions for burstable performance Instances.</p> <p>Lower Costs Today by Right-Sizing Your EC2 Instance Amazon EC2 T3 Instances.</p>"},{"location":"AWS/remote-ec2/#ec2-ssh-error","title":"EC2 ssh error","text":"<p>EC2 SSH Error</p>"},{"location":"AWS/s3-bucket/","title":"S3 bucket","text":"<p>Check with other types of services https://aws.amazon.com/products/storage/</p> <p>This Type of  service provides</p> <p>Info</p> <p>AWS Storage Gateway</p> <p>connecting s3 via <code>NFS</code> AND <code>SMB</code>  service </p>"},{"location":"AWS/s3-bucket/#s3-glacier-vault-lock","title":"S3 Glacier Vault Lock","text":""},{"location":"AWS/s3-bucket/#presigned-urls-to-share-access-to-your-s3-bucket","title":"presigned URLs to share access to your S3 bucket","text":"<pre><code>allow users to download only their own files.\nuse presigned URLs to share access to your S3 buckets. When you create a presigned URL, you associate it with a specific action and an expiration date. Anyone who has access to the URL can perform the action embedded in the URL as if they were the original signing user.\n</code></pre>"},{"location":"AWS/soci/","title":"Soci","text":"<p>https://www.youtube.com/watch?v=N2CLxDgMDDo&amp;t=1568s</p>"},{"location":"AWS/waf/","title":"WAF-AWS","text":"<p>For implementations  aws-waf-security aws.com. </p> <p>aws.com. </p> <p>aws.com. </p> <p>Athena Query   domain.com. </p> <p>aws.com. </p> <p>aws.com. </p> <p>domaawsin.com. </p> <p>waf rate_based rule aws.com. </p>"},{"location":"AWS/workshops/","title":"ecsworksop","text":"<p>https://ecsworkshop.com/</p>"},{"location":"AWS/iam/policies/code.commit/","title":"Code.commit","text":"<p>Code Commmit</p> <pre><code>    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"VisualEditor0\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"codecommit:*\",\n                \"Resource\": [\n                    \"arn:aws:codecommit:us-east-1:123456789123:repo-name\",\n                    \"arn:aws:codecommit:us-east-1:123456789123:repo-name\",\n                ],\n                \"Condition\": {\n                    \"StringEqualsIfExists\": {\n                        \"codecommit:References\": [\n                            \"refs/heads/master\",\n                            \"refs/heads/main\",\n                            \"refs/heads/mani\",\n                            \"refs/heads/mack\",\n                            \"refs/heads/eswar\"\n                        ]\n                    },\n                    \"IpAddress\": {\n                        \"aws:SourceIp\": [\n                            \"192.168.0.100\",\n                            \"192.168.0.101\"\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n</code></pre>"},{"location":"AWS/iam/policies/lambda-assume-role/","title":"Lambda assume role","text":"<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"AWS/iam/policies/lambda-assume-role/#allow-access-to-lambda-to-access-resources","title":"Allow Access To Lambda To Access resources","text":"<p>Master account</p> <p>lambda-ec2-fleet-management</p> <p>ec2-start-stop-lambda-role</p> <p>======= Add service policy</p> <p>AmazonEC2FullAccess  ServiceQuotasFullAccess </p>"},{"location":"AWS/iam/policies/lambda-assume-role/#trusted-entities","title":"Trusted Entities","text":"<p>1a23421 (Rebase_Edited_Update) {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": { &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD                 \"Service\": \"lambda.amazonaws.com\" =======                 \"AWS\": \"arn:aws:iam::root-acccount-number:role/ec2-start-stop-lambda-role\" 1a23421 (Rebase_Edited_Update)             },             \"Action\": \"sts:AssumeRole\"         }     ] &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD }</p> <p>AdministratorAccess  AmazonSNSFullAccess  AWSMarketplaceFullAccess Tag </p> <p>date 19/06/2023</p> <p>Project  Ec2-Fleet-Management</p> <p>==================================</p> <p>Slave account</p> <p>ec2-fleet-management-lambda-assume-role</p> <p>Description :- Ec2-Fleet-Management By Lambda From Master Account</p> <p>{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": {                 \"AWS\": \"arn:aws:iam::362778997593:role/lambda-ec2-fleet-management\"             },             \"Action\": \"sts:AssumeRole\"         }     ] }</p> <p>permission</p> <p>AmazonEC2FullAccess ServiceQuotasFullAccess</p> <p>Tag </p> <p>date 19/06/2023</p> <p>Project  Ec2-Fleet-Management ======= }</p> <p>1a23421 (Rebase_Edited_Update)</p>"},{"location":"AWS/iam/policies/switch-role/","title":"Switch role","text":"<p>Create a Role</p> <p>Put  In Trusted Relationship</p> <p>{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": {                 \"AWS\": \"arn:aws:iam::362778997593:root\"             },             \"Action\": \"sts:AssumeRole\",             \"Condition\": {}         }     ] }</p> <p>Add All These In Permission Block</p> <p>AdministratorAccess AWS managed  Billing AWS managed  AWSBillingConductorFullAccess</p> <p>Allow Access on Belhahf for root account</p> <p>tag</p>"},{"location":"AWS-boto/Assume-role-lambda/","title":"Assume role lambda","text":""},{"location":"AWS-boto/Assume-role-lambda/#configure-a-lambda-function-to-assume-an-iam-role-in-another-aws-account","title":"Configure a Lambda function to assume an IAM role in another AWS account","text":"<p>IAM-Role Assume role link</p> <p>Youtube Video_link</p> <ul> <li> <p>I need my AWS Lambda function to assume an AWS Identity and Access Management (IAM) role in another AWS account. How do I set that up?</p> </li> <li> <p>Short description    To have your Lambda function assume an IAM role in another AWS account, do the following:</p> </li> <li> <p>Configure your Lambda function's execution role to allow the function to assume an IAM role in another AWS account.</p> </li> <li>Modify your cross-account IAM role's trust policy to allow your Lambda function to assume the role.</li> <li>Add the AWS Security Token Service (AWS STS) AssumeRole API call to your Lambda function's code.</li> <li> <p>Note: A Lambda function can assume an IAM role in another AWS account to do either of the following:</p> </li> <li> <p>Access resources\u2014For example, accessing an Amazon Simple Storage Service (Amazon S3) bucket.</p> </li> <li> <p>Do tasks\u2014For example, starting and stopping instances. Resolution Note: The following example procedure references two different types of AWS accounts:</p> </li> <li> <p>A home account that hosts the Lambda function ( 111111111111).</p> </li> <li> <p>A cross-account that includes the IAM role that the Lambda function assumes (222222222222) The procedure assumes:</p> </li> <li> <p>You have created the IAM role that you want to use in the cross-account (222222222222)</p> </li> <li>Configure your Lambda function's execution role to allow the function to assume an IAM role in another AWS account</li> <li>Add the following policy statement to your Lambda function's execution role (in account 111111111111) by following the instructions in Adding and removing IAM identity permissions:</li> </ul> <p>Important: Replace 222222222222 with the AWS account ID of the cross-account role that your function is assuming. Replace role-on-source-account with the assumed role's name.</p> <p>Iam</p> Role <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": {\n        \"Effect\": \"Allow\",\n        \"Action\": \"sts:AssumeRole\",\n        \"Resource\": \"arn:aws:iam::222222222222:role/role-on-source-account\"\n    }\n}\n</code></pre> <ul> <li>Modify your cross-account IAM role's trust policy to allow your Lambda function to assume the role</li> <li> <p>Add the following policy statement to your cross-account IAM role's trust policy (in account 222222222222) by following the instructions in Modifying a role trust policy (console):</p> </li> <li> <p>Important: Replace 111111111111 with the AWS account ID of the account that your Lambda function is in. Replace my-lambda-execution-role with the name of your function's    execution role.</p> </li> </ul> <p>Example</p> Unordered List <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111111111111:role/my-lambda-execution-role\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Add the AWS STS AssumeRole API call to your Lambda function's code</li> <li>Add the AWS STS AssumeRole API call to your function's code by following the instructions in Configuring Lambda function options.</li> </ul> <p>Note: The AWS STS AssumeRole API call returns credentials that you can use to create a service client. By using this service client, your Lambda function has the permissions granted to it by the assumed role. For more information, see assume_role in the AWS SDK for Python (Boto 3) documentation.</p> <ul> <li>Python function code example that includes the AWS STS AssumeRole API call</li> </ul> <p>Important: Replace 222222222222 with the AWS account ID of the cross-account role that your function is assuming. Replace role-on-source-account with the assumed role's name.</p> <p>Python</p> Lambda Function <pre><code>import boto3\n\ndef lambda_handler(event, context):\n\n    sts_connection = boto3.client('sts')\n    acct_b = sts_connection.assume_role(\n        RoleArn=\"arn:aws:iam::222222222222:role/role-on-source-account\",\n        RoleSessionName=\"cross_acct_lambda\"\n    )\n\n    ACCESS_KEY = acct_b['Credentials']['AccessKeyId']\n    SECRET_KEY = acct_b['Credentials']['SecretAccessKey']\n    SESSION_TOKEN = acct_b['Credentials']['SessionToken']\n\n    # create service client using the assumed role credentials, e.g. S3\n    client = boto3.client(\n        's3',\n        aws_access_key_id=ACCESS_KEY,\n        aws_secret_access_key=SECRET_KEY,\n        aws_session_token=SESSION_TOKEN,\n    )\n\n    return \"Hello from Lambda\"\n</code></pre> <p>Troubleshoot</p> <p>IAM-Assume-Role-Errorblog_link</p> <p>Youtube video_link</p>"},{"location":"AWS-boto/references/","title":"References","text":"<ul> <li> <p>Assign Private_IP-Address AWS_Link</p> </li> <li> <p>EC2-Using-Boto3-Pyhton Boto-3-Python</p> </li> <li> <p>Using-Bot-3 BOTO-3</p> </li> <li> <p>Json Formatter @ CuriousConcept</p> </li> <li> <p>Secruity_Group_Boto-3 SG_BOTO-3</p> </li> </ul>"},{"location":"EKS/Service_mesh/","title":"Service_mesh","text":"<p>getting-started-with-istio-on-amazon-eks</p> <p>amazon-eks-with-isti</p> <p>aws-samples</p> <p>istio_eksworkshop</p>"},{"location":"EKS/ec2-iam/EC2--IAM_access/","title":"Cross Account Access Policy.","text":""},{"location":"EKS/ec2-iam/EC2--IAM_access/#in-master-account-a","title":"IN MASTER ACCOUNT A","text":"<pre><code>Add Required Policies to Roles\n\nInline Policy\n\n=== \"Role Name: Mk-ec2\"\n    ```\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": \"sts:AssumeRole\",\n                \"Resource\": \"arn:aws:iam::331911183167:role/Mk-ec2-policy\"\n            }\n        ]\n    }\n    ```\n\nTrusted entities\n\n=== \"Trusted entities\"\n    ```\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"ec2.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            }\n        ]\n    }\n    ```\n</code></pre>"},{"location":"EKS/ec2-iam/EC2--IAM_access/#in-slave-account-b","title":"IN SLAVE ACCOUNT B","text":"<pre><code>Add Required Policies to Roles\n\nTrusted entities\n\n=== \"Trusted entities\"\n    ```\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"AWS\": \"arn:aws:iam::362778997593:role/Mk-ec2\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            }\n        ]\n    }    \n    ```\n</code></pre>"},{"location":"EKS/ec2-iam/EC2--IAM_access/#to-add-it-in-aws-config-file-location","title":"To Add It in aws config file location","text":"<pre><code>Add content in .aws/config  if fole is not Present create it.\n\n=== \"Trusted entities\"\n    ```\n    [profile seeding]\n    role_arn = arn:aws:iam::331911183167:role/Mk-ec2-policy\n    credential_source = Ec2InstanceMetadata\n    ```\n</code></pre>"},{"location":"EKS/ec2-iam/EC2--IAM_access/#run-commands-to-update-profile-creditionals-to-ec2","title":"Run Commands to update Profile creditionals to ec2","text":"<pre><code>aws sts get-caller-identity --profile seeding\n</code></pre>"},{"location":"EKS/ec2-iam/EC2--IAM_access/#to-allow-eks-access-to-this-profile","title":"To Allow EKS access to this Profile","text":"<pre><code>![Eks cluster using iam Referenced url](https://antonputra.com/kubernetes/add-iam-user-and-iam-role-to-eks/#add-iam-user-to-eks-cluster)\n\nIf You create a Cluster using IAM User You Don't Need to do this,\nIf You have accessing using Role without user, use this Below Method.\n\n=== \"\"\n    ``` bash\n    aws eks update-kubeconfig --region us-east-1 --name seeding --profile seeding        \n\n    kubectl edit -n kube-system configmap/aws-auth\n\n    ...\n    mapUsers: |\n        - rolearn: arn:aws:iam::331911183167:role/Mk-ec2-policy\n        username: Mk-ec2-policy\n        groups:\n        - system:masters\n    ...\n\n    ```\n    apiVersion: v1\n    data:\n    mapRoles: |\n      - groups:\n        - system:bootstrappers\n        - system:nodes\n        rolearn: arn:aws:iam::331911183167:role/AmazonEKSNodeRole\n        username: system:node:{{EC2PrivateDNSName}}\n      - groups:\n        - system:bootstrappers\n        - system:nodes\n        - system:node-proxier\n        rolearn: arn:aws:iam::331911183167:role/AmazonEKSFargatePodExecutionRole\n        username: system:node:{{SessionName}}\n      - rolearn: arn:aws:iam::331911183167:role/Mk-ec2-policy\n        username: Mk-ec2-policy\n        groups:\n        - system:masters\n\n    ```\n\n\n    aws eks update-kubeconfig \\\n    --region us-east-1 \\\n    --name seeding \\\n    --profile seeding\n    ```\n</code></pre>"},{"location":"Helm_chart/Helm_chart/","title":"Helm_chart","text":"<p>Helm go template function https://helm-playground.com/cheatsheet.html</p>"},{"location":"Kubernetes/CheatSheet/","title":"Cheatsheet","text":"<p>kubectl get - list resources kubectl describe - show detailed information about a resource kubectl logs - print the logs from a container in a pod kubectl exec - execute a command on a container in a pod You can use these commands to see when applications were deployed, what their current statuses are, where they are running and what their configurations are.</p>"},{"location":"Kubernetes/CheatSheet/#every-kubernetes-node-runs-at-least","title":"Every Kubernetes Node runs at least:","text":"<p>Kubelet, a process responsible for communication between the Kubernetes control plane and the Node; it manages the Pods and the containers running on a machine. A container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.</p>"},{"location":"Kubernetes/CheatSheet/#to-login-into-pod","title":"To login into pod","text":"<p>kubectl exec -ti $POD_NAME -- bash</p>"},{"location":"Kubernetes/CheatSheet/#to-scale-a-application","title":"To scale a application","text":"<p>kubectl scale deployments/kubernetes-bootcamp --replicas=4</p>"},{"location":"Kubernetes/CheatSheet/#to-get-details-of-scaled-pods","title":"To get details of scaled pods","text":"<p>kubectl get pods -o wide</p>"},{"location":"Kubernetes/Cluster.Prerequisite/","title":"Keda Autoscaler","text":"<pre><code>It is Important In-order to scale pods based on number of request present in selenium-grid queue.\n</code></pre>"},{"location":"Kubernetes/Cluster.Prerequisite/#_1","title":"Cluster.prerequisite","text":"<pre><code>    install https://keda.sh/ plugin  in kube\n    helm repo add kedacore https://kedacore.github.io/charts\n    kubectl create namespace keda   # [ Create only if Needed]\n    helm install keda kedacore/keda --namespace keda  # [ Dont use Namspace     if selenium is running in default workspace]\n    helm install -f values.yaml docker-selenium\n</code></pre>"},{"location":"Kubernetes/Cluster.Prerequisite/#installing-the-kubernetes-metrics-server","title":"Installing the Kubernetes Metrics Server","text":"<pre><code>https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nkubectl get deployment metrics-server -n kube-system\n</code></pre>"},{"location":"Kubernetes/Links/","title":"autoscaling microservices with kubernetes event driven autoscaler scaler","text":"<p>https://medium.com/cuddle-ai/auto-scaling-microservices-with-kubernetes-event-driven-autoscaler-keda-8db6c301b18</p>"},{"location":"Kubernetes/Links/#links","title":"Links","text":"<p>For full documentation visit azuredevopslabs.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit amazon.com. </p> <p>For full documentation visit miraclemill.com.</p> <p>For full documentation visit github.com. </p> <p>For full documentation visit anthology.com. </p>"},{"location":"Kubernetes/Lists/","title":"TODO   VPC Peering","text":""},{"location":"Kubernetes/Lists/#todo-helm-chart","title":"TODO   Helm Chart","text":"<pre><code>    Download helm chart\n    remove other edge&amp;firefox&amp;node's from helm chart\n\n    check this repo for scaling pods {\n        https://github.com/SeleniumHQ/docker-selenium/issues/1688\n        https://github.com/prashanth-volvocars/docker-selenium/blob/auto-scaling/charts/selenium-grid/values.yaml\n        }\n</code></pre>"},{"location":"Kubernetes/Lists/#todo-kube-ok","title":"TODO   kube  ok","text":""},{"location":"Kubernetes/Lists/#todo","title":"TODO","text":""},{"location":"Kubernetes/Lists/#keda","title":"Keda","text":""},{"location":"Kubernetes/Lists/#selenium-autoscaler","title":"selenium autoscaler","text":"<pre><code>    install https://keda.sh/ plugin  in kube\n    helm repo add kedacore https://kedacore.github.io/charts\n    kubectl create namespace keda\n    helm install keda kedacore/keda --namespace keda\n    helm install -f values.yaml docker-selenium\n</code></pre>"},{"location":"Kubernetes/Lists/#notes","title":"NOTES","text":"<pre><code>    try graphql  or /status\n</code></pre>"},{"location":"Kubernetes/Lists/#autoscaling","title":"Autoscaling","text":"<pre><code>    pods are scale-in and scale-out by keda,\n    nodes are scaled out and scaled-in by autoscaling group by aws\n    By how individual grid isolated components scale's by default it have only 1 replica enabled in helm\n    Increase resource size for pod's\n</code></pre>"},{"location":"Kubernetes/Topics/","title":"Topics","text":"<p>serviceaccount . helmcharts resource limit  readiness liveliness</p> <p>archict containzered</p> <p>clusterarchitech in</p> <p>controller * workload  deamon replica</p> <p>internal policy</p> <p>storage presitance config maps *  secrets *  how to create secrets  organize clustr access using kubeconfig</p>"},{"location":"Kubernetes/Topics/#topic-suggested-by-meeting-aws","title":"Topic suggested by Meeting aws","text":"<p>use service account for pod to allow access to s3 &amp; use boto3 to file object get from s3.</p> <p>costallocation tag for 24/7 running server  and 8 hrs shutdown server's all tag's for all server</p> <p>Wait for 30 days to check the recommendation, for server which is shuttuing down daily.</p> <p>share the screen shot ok existing eks cluster and clusterIP</p> <p>[shared resources &amp; Links]</p> <p>https://github.com/awslabs/amazon-eks-ami/blob/master/files/max-pods-calculator.sh https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html https://docs.aws.amazon.com/eks/latest/userguide/fargate-profile.html https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-1-service-and-ingress-resources/</p>"},{"location":"Kubernetes/api/","title":"APi","text":"<p>Mostly used two  types</p> <p>REST, SOAP</p> <p>reprenstlational state tranfer (all purpose web,) xml,html,json,text easy format own mostly json (javascript object notation) Use any format worldwide used json, json is lightweight.</p> <p>simple object access protocol  (messaging system) xml format is heavy.</p> <p>soap use websocket wsd</p> <p>rest api use's http api portocoal use asyncrinious</p> <p>rest is a syncrinious stateless (rest is client server architecture)</p> <ul> <li>crud</li> </ul> <p>post create get read put update delete</p> <p>--- REST --- limited resouces and bandwidth</p>"},{"location":"Kubernetes/basics/","title":"Kube basics","text":"<p>## Service Account In Kubernetes  'https://medium.com/the-programmer/working-with-service-account-in-kubernetes-df129cb4d1cc'  https://www.cncf.io/blog/2019/05/10/kubernetes-core-concepts/</p> <p>Service Account: It is used to authenticate machine level processes to get access to our Kubernetes cluster. The API server is responsible for such authentication to the processes running in the pod.</p> <p>For Example:     An application like Prometheus accessing the cluster to monitor it is a type of service account</p> <pre><code>So,\n\nA service account is an identity that is attached to the processes running within a pod.\n</code></pre> <p>When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.</p> <p>Case 1:</p> <pre><code> My Web Page which has a list of items to be displayed, this data needs to be fetched from an API server hosted in the Kubernetes cluster as shown above in the figure. To do so, we need to a service account that will be enabled by cluster API servers to authenticate and access the data from the cluster servers.\n</code></pre> <p></p>"},{"location":"Kubernetes/basics/#helm-chart","title":"helm chart","text":"<p>the package manager for Kubernetes Helm allows you to add variables and use functions inside your template files. This makes it perfect for scalable applications that'll eventually need to have their parameters changed.</p>"},{"location":"Kubernetes/basics/#three-big-concepts","title":"Three Big Concepts","text":"<p>A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file.</p> <p>A Repository is the place where charts can be collected and shared. It's like Perl's CPAN archive or the Fedora Package Database, but for Kubernetes packages.</p> <p>A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times into the same cluster. And each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name.</p> <p>With these concepts in mind, we can now explain Helm like this:</p> <p>Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories.</p>"},{"location":"Kubernetes/basics/#helm-install-installing-a-package","title":"'helm install': Installing a Package","text":"<p>To install a new package, use the helm install command. At its simplest, it takes two arguments: A release name that you pick, and the name of the chart you want to install.</p>"},{"location":"Kubernetes/basics/#resource-limit","title":"resource limit","text":"<p>https://sysdig.com/blog/kubernetes-limits-requests/</p>"},{"location":"Kubernetes/commands/","title":"connect to existing Cluster","text":""},{"location":"Kubernetes/commands/#to-update-existing-kubectl-cluster-to-local-kubeconfig-for-connecting","title":"To update existing kubectl cluster To local kubeconfig For Connecting","text":"<pre><code>  aws eks update-kubeconfig --name &lt;cluster-name&gt; --profile \n  ```\n  ```bash\n  aws eks update-kubeconfig --name seeding --profile seeding --region us-east-1\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-service","title":"To Get Service","text":"<p><pre><code>  kubectl get svc\n</code></pre> <pre><code>  eksctl create cluster --name selenium --region ap-south-1\n  ```\n  ```bash\n  eksctl create cluster \"selenium\" --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n  ```\n  ```bash\n  eksctl register cluster --name selenium --provider other --region us-east-1\n  ```\n```bash\n  aws eks update-kubeconfig --region us-east-1 --name selenium\n</code></pre></p>"},{"location":"Kubernetes/commands/#to-delete-existing-kubectl-config","title":"To delete existing kubectl config","text":"<pre><code>  rm ~/.kube/config\n\n  helm install selenium-grid docker-selenium/selenium-grid\n  kubectl get all -n selenium-grid\n  kubectl get services\n</code></pre>"},{"location":"Kubernetes/commands/#to-connect-to-internal-network-using-busybox","title":"To connect to internal network using busybox","text":"<pre><code>  kubectl run -i --tty busybox --image=busybox --restart=Never -- sh\n</code></pre>"},{"location":"Kubernetes/commands/#for-centos-pod","title":"For centos pod","text":"<pre><code>  kubectl run -i --tty centos --image=centos --restart=Never -- sh\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-all-resource-details","title":"To get all resource details","text":"<pre><code>  kubectl get all\n</code></pre>"},{"location":"Kubernetes/commands/#to-delete-the-deployment","title":"To delete the deployment","text":"<pre><code>  kubectl delete deployments -l app=selenium-edge-node\n</code></pre>"},{"location":"Kubernetes/commands/#to-describe-events-in-running-pods","title":"To Describe Events In Running Pods","text":"<pre><code>  kubectl describe pods -l app=selenium-chrome-node | grep Events -A4\n</code></pre>"},{"location":"Kubernetes/commands/#to-force-delete-a-pod","title":"To Force Delete A Pod","text":"<pre><code>  kubectl delete pod selenium-chrome-node --grace-period=0 --force\n  kubectl delete pod selenium-edge-node --grace-period=0 --force\n  kubectl delete pod selenium-firefox-node --grace-period=0 --force\n  kubectl delete pod selenium-hub-c6c94c6c4-h558k --grace-period=0 --force\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-status-of-kube-system","title":"To get status of kube system","text":"<pre><code>  kubectl get pods -n=kube-system | grep coredns\n</code></pre>"},{"location":"Kubernetes/commands/#to-scale-pod-replicaset","title":"To scale pod replicaset","text":"<pre><code>  kubectl scale --replicas=5 replicaset/selenium-chrome-node-7bf4f8dc77\n  kubectl scale deployment.apps/selenium-node-chrome-node --replicas=1\n</code></pre>"},{"location":"Kubernetes/commands/#scale-deployment","title":"scale deployment","text":"<pre><code>  kubectl scale deployment/selenium-chrome-node --replicas=2\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-the-value-replicaset","title":"To get the value replicaset","text":"<pre><code>  kubectl get rs selenium-chrome-node-5f44bffc9b -o jsonpath=\"{.status.replicas} {.status.availableReplicas}\"\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-replica-status","title":"To get replica status","text":"<pre><code>  kubectl get rs\n</code></pre>"},{"location":"Kubernetes/commands/#run-the-following-command-to-forward-a-local-port-to-the-service","title":"Run the following command to forward a local port to the service:","text":"<pre><code>  kubectl port-forward service/selenium-hub 4444:4444\n</code></pre>"},{"location":"Kubernetes/commands/#to-get-running-resource-to-yaml","title":"To get running resource to yaml","text":"<pre><code>  crd = CustomResourceDefinition \n  rs  = replicaset\n  kubectl get crd &lt;CRD-NAME&gt; -o yaml\n</code></pre>"},{"location":"Kubernetes/commands/#if-crd-unable-to-delete-remove-the-crd-using-kubectl-edit","title":"If CRD unable to Delete remove the crd using kubectl edit","text":"<p>link-to-delete-crd[https://learn.microsoft.com/en-us/answers/questions/602466/custom-crds-not-getting-deleted-in-aks-cluster-how] <pre><code>  kubectl edit crd crd-name   #  Remove finalizers from it and save\n</code></pre></p>"},{"location":"Kubernetes/commands/#eks-cluster-with-command","title":"eks cluster with command","text":""},{"location":"Kubernetes/commands/#working-with-fargateimage-is-downloading-running","title":"working with fargate,image is downloading &amp; running","text":"<p><pre><code> eksctl create cluster --name seeding --region us-east-1 --version 1.26 --fargate --profile\n</code></pre>  seeding    Next:       allow 80 port in security group for loadbalancer access</p>"},{"location":"Kubernetes/commands/#to-get-metric-server","title":"To get metric server","text":"<pre><code>  kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1\"\n</code></pre>"},{"location":"Kubernetes/commands/#to-find-which-namespce-if-metric-in","title":"To Find which Namespce if metric in","text":"<pre><code>  kubectl get scaledobject selenium-chrome-scaledobject -n default -o jsonpath={.status.externalMetricNames}\n</code></pre>"},{"location":"Kubernetes/commands/#describe-scaledobject","title":"describe scaledobject","text":"<pre><code>  kubectl describe scaledobject\n</code></pre>"},{"location":"Kubernetes/commands/#to-create-node-group","title":"To create node group","text":"<pre><code>  eksctl create nodegroup --cluster=seeding \\\n  --name=node2 \\\n  --node-type=c5.2xlarge \\\n  --nodes=3 \\\n  --nodes-min=3 \\\n  --nodes-max=3 \\\n  --node-volume-size=20 \\\n  --ssh-access \\\n  --ssh-public-key=6548652153 \\\n  --managed \\\n  --region ap-south-1 \n  ```\n\n\n### To delete node group\n```bash\n  eksctl delete nodegroup --cluster=seeding --region ap-south-1\n  eksctl delete --cluster=seeding --region ap-south-1\n</code></pre>"},{"location":"Kubernetes/commands/#to-proxypass-service","title":"To Proxypass service","text":"<pre><code>  kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090\n\n  kubectl kubernetes port-forward service/kubernetes 9090\n</code></pre>"},{"location":"Kubernetes/commands/#to-connect-to-ssh-to-pod","title":"To connect to ssh to pod","text":"<pre><code>  kubectl exec --stdin --tty busybox -- /bin/bash\n  kubectl exec --it  busybox -n namespace -- /bin/bash\n</code></pre>"},{"location":"Kubernetes/commands/#field-selector","title":"Field selector","text":""},{"location":"Kubernetes/commands/#to-delete-all-pods","title":"To delete all pods","text":""},{"location":"Kubernetes/commands/#getting-specific-evicted-is-not-working","title":"Getting Specific Evicted is not working","text":"<pre><code>kubectl delete pods --field-selector status.phase=Failed\n</code></pre> <pre><code>kubectl logs -n {namespace}  (podname) --tail=100\n</code></pre>"},{"location":"Kubernetes/commands/#if-above-not-working-using-previousfalse","title":"If above not working using --previous=false","text":"<pre><code>kubectl logs -p dbc-ddl-service-ddc4cf6f9-6dqx8 -n {namespace} --previous=false --tail=50\n</code></pre> <pre><code>kubectl logs -p (podname)  --since=30m --timestamps \n</code></pre> <pre><code>k get events -n {namespace} --field-selector involvedobject.name={pod_name} --sort-by='.metadata.creationTimestamp'\n</code></pre> <pre><code>k describe pod {pod_name} -n {namespace}\n</code></pre>"},{"location":"Kubernetes/commands/#get-the-count-of-the-pods","title":"Get the count of the pods","text":"<pre><code>kubectl get pods -n {namespace} | grep \"dbc-ddl-service\" | wc -l\n</code></pre>"},{"location":"Kubernetes/commands/#scale-replica","title":"scale replica","text":"<pre><code>kubectl scale  deployment mysql --replicas=3\n</code></pre>"},{"location":"Kubernetes/commands/#delete-pods-where-status-is-evicted","title":"Delete pods where status is Evicted","text":"<pre><code>kubectl delete pod $(kubectl get pods  --field-selector=status.phase=Failed -o jsonpath='{.items[?(@.status.reason==\"Evicted\")].metadata.name}')\n</code></pre>"},{"location":"Kubernetes/commands/#rollout-deployment","title":"rollout deployment","text":"<pre><code>kubectl rollout restart deployment {podname} -n lifion\n</code></pre>"},{"location":"Kubernetes/commands/#get_the-status-of-container_restart-count","title":"get_the status of container_restart count","text":"<pre><code>kubectl get pods --sort-by='.status.containerStatuses[0].restartCount'\n</code></pre>"},{"location":"Kubernetes/initialize.commands/","title":"Initialize.commands","text":"<p>Turn off swap complusory kubeadm init --apiserver-advertise-address {public_ip_or_private_ip_for api server} --pod-network-cidr= {Docker_container_network_subnet}</p> <p>Install network flannel wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml Change it in netowrk section for custom docker network subnet kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml</p> <p>kubectl get nodes kubectl get pods -n kube-system -o wide</p>"},{"location":"Kubernetes/initialize.commands/#script-a-logtxt-for-session-logging","title":"script -a log.txt   For session Logging","text":""},{"location":"Kubernetes/Cluster_Setup/Local_Cluster_Setup/","title":"K8's Master &amp; Worker Node Local Setup:","text":""},{"location":"Kubernetes/selenium/Cluster.Prerequisite/","title":"Keda Autoscaler","text":"<pre><code>It is Important In-order to scale pods based on number of request present in selenium-grid queue.\n</code></pre>"},{"location":"Kubernetes/selenium/Cluster.Prerequisite/#_1","title":"Cluster.prerequisite","text":"<pre><code>    install https://keda.sh/ plugin  in kube\n    helm repo add kedacore https://kedacore.github.io/charts\n    kubectl create namespace keda   # [ Create only if Needed]\n    helm install keda kedacore/keda --namespace keda  # [ Dont use Namspace     if selenium is running in default workspace]\n    helm install -f values.yaml docker-selenium\n</code></pre>"},{"location":"Kubernetes/selenium/Cluster.Prerequisite/#installing-the-kubernetes-metrics-server","title":"Installing the Kubernetes Metrics Server","text":"<pre><code>https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\nkubectl get deployment metrics-server -n kube-system\n</code></pre>"},{"location":"Kubernetes/selenium/Readme/","title":"Readme","text":"<p>Install Cluser prerequisite Content into Eks cluster</p>"},{"location":"Mkdocs/reference/","title":"markdown-reference","text":"CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <p>To</p> To link files from location <pre><code>echo \"Enter Group Name To Create for sftp user\"\n\n# read sftpgroup\n# sftpgroup=\"\"\nsftpgroup=\"sftpgroup_resticted\"\ngroupadd $sftpgroup\n\necho \"Entered User names\" \necho $@\na=(\"$@\")\n\necho \"Permission Updated for respective User's\"\n\nfor names in \"${!a[@]}\"\ndo\n     Username=${a[$names]}\n    #  echo \"$nam\"\n     ls -ld /home/$Username\n    chown root:$TOKEN /home/$Username\n    chmod 775 /home/$Username\n    sudo usermod -a -G $sftpgroup $Username\ndone\n\necho \"Updating SSh config file for sftp users\"\n\ncat &lt;&lt;EOF &gt;&gt; /5\nMatch Group sftpuser\n    ChrootDirectory /home/%u\n    ForceCommand internal-sftp\n    X11Forwarding no\n    AllowTcpForwarding no\nEOF\n\nservice sshd restart\n\nb=\"$?\"\necho \"$b\"\nif [ $b == 0 ]\nthen   \n    echo \"Update Done\"\nelse\n    echo \"Update not done\"\nfi\n</code></pre> <p>Example</p> Unordered ListOrdered List <pre><code>* Sed sagittis eleifend rutrum\n* Donec vitae suscipit est\n* Nulla tempor lobortis orci\n</code></pre> <pre><code>1. Sed sagittis eleifend rutrum\n2. Donec vitae suscipit est\n3. Nulla tempor lobortis orci\n</code></pre> <p></p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> </ul> <p><code>elect from table;</code></p>"},{"location":"Mkdocs/reference/#2","title":"2","text":"<p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p> <p>Highlighting text\u00b6</p> <ul> <li>This was marked</li> <li>This was inserted</li> <li>This was deleted</li> </ul> Tab 1Tab 2Unordered listOrdered list <p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li>:man_raising_hand: I'm an annotation!</li> </ol> <p>Phasellus posuere in sem ut cursus (1)</p> <ol> <li>:woman_raising_hand: I'm an annotation as well!</li> </ol> <ul> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ul> <ol> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ol> <p>Info</p> <p>Installing Serverless Framework as a standalone binary</p> <p>For full documentation visit domain.com. </p> <p>Graph</p> <p><code>mermaid graph LR   A[Start] --&gt; B{Error?};   B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];</code></p> <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul>"},{"location":"Mysql/mysql.cnf/","title":"Mysql.cnf","text":"<p>Mysql</p> <pre><code>```\n[mysqld]\nport                            = 3306\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\ntmpdir                          = /var/lib/mysql/mysql_temp  \nslave-skip-errors=1062,1146,1053,1064,1032,1677\nslow_query_log                  = ON\nlong_query_time                 = 10\nslow_query_log_file             = /var/log/mysqld/slow.log\nread_buffer_size                = 8M \nsort_buffer_size                = 8M \n```\n</code></pre>"},{"location":"Mysql/mysql.cnf/#disabling-symbolic-links-is-recommended-to-prevent-assorted-security-risks","title":"Disabling symbolic-links is recommended to prevent assorted security risks","text":"<p>symbolic-links=0</p> <p>user=mysql</p>"},{"location":"Mysql/mysql.cnf/#innodb","title":"INNODB","text":"<p>Note</p> <pre><code>innodb-buffer-pool-size        = 24M\n\ninnodb-f lush-method            = O_DIRECT\n\ninnodb-flush-log-at-trx-commit = 2\n\ninnodb_buffer_pool_instances   = 8\n\ninnodb_log_file_size           = 50M\n\ninnodb_log_buffer_size         = 32M\n\ninnodb_thread_concurrency      = 8\n</code></pre>"},{"location":"Mysql/mysql.cnf/#caches-and-limits","title":"CACHES AND LIMITS","text":"<p>Note</p> <pre><code>    #tmp-table-size                 = 32M\n\n    max-heap-table-size             = 32M\n\n    query_cache_type                = 1\n\n    query_cache_size                = 800M\n\n    max_connections                 = 1700\n\n    wait_timeout                    = 800\n\n    thread_cache_size               = 512\n\n    open-files-limit                = 65535\n\n    table-definition-cache          = 1024\n\n    table-open-cache                = 2048\n</code></pre>"},{"location":"Mysql/mysql.cnf/#tuning","title":"TUNING","text":"<p>Note</p> <pre><code>    max-allowed-packet             = 1G\n\n    max-connect-errors             = 1000\n</code></pre>"},{"location":"Mysql/mysql.cnf/#binlog","title":"Binlog","text":"<p>Note</p> <pre><code>    log-bin                        = /var/lib/mysql/mysql-bin\n\n    expire-logs-days               = 14\n\n    sync-binlog                    = 1\n</code></pre>"},{"location":"Mysql/mysql.cnf/#_2","title":"Mysql.cnf","text":""},{"location":"Mysql/mysql.cnf/#_3","title":"Mysql.cnf","text":"<ul> <li> <p>[mysqld_safe]</p> </li> <li> <p>log-error=/var/log/mysqld.log</p> </li> <li> <p>pid-file=/var/run/mysqld/mysqld.pid <pre><code>[client]\n\nport                    = 3306\n\nsocket                  = /var/lib/mysql/mysql.sock\n</code></pre></p> </li> </ul>"},{"location":"Mysql/mysql.cnf/#tuining-mysql","title":"Tuining mysql","text":"<pre><code>read_buffer_size                = 62M\n\nsort_buffer_size                = 62M\n\ninnodb_buffer_pool_size         = 24M\n\nread_buffer_size                = 62M --- 1MB for every 1GB of RAM\n\nsort_buffer_size                = 62M --- 1MB for every 1GB of RAM\n\ninnodb_buffer_pool_size         = 24M\n</code></pre>"},{"location":"NAS/Tools/","title":"Tools","text":""},{"location":"NAS/Tools/#_1","title":"Tools","text":"<ul> <li> <p>PromaxT</p> </li> <li> <p>TrueNax</p> </li> </ul>"},{"location":"Nginx/","title":"Readme","text":"<p>hi</p>"},{"location":"Projects/Redirection/Notes/","title":"Notes","text":""},{"location":"Projects/Redirection/Notes/#points-to-note","title":"Points to Note:","text":"<p>Notes</p> <pre><code>Existing archicture does not have load balancer when sudden request come, The server may unable to handle it, \nrequest are unable server and drop, we don't have track of dropped request\n\nTo solve this we can use  Load balancer it basically does split the request when a instance or service is avaible,LB Make sure that the service is avaiable via Health check's remember it from aws load balacner column to check for  specific request.\n</code></pre>"},{"location":"Projects/Redirection/Refrences/","title":"Refrences","text":"<p>Nginx for Partner Amazon-Web-ServicesNginx</p> <p>Nginx for Ingrss-Controllernginx</p> <p>Keywords <pre><code>\"deploy production grade nginx in eks\"\n</code></pre></p>"},{"location":"Python/basics/","title":"Python skeleton","text":"<p>Notes</p> <p>Is a basic structure of a Python program that can be used as a starting point for writing new code. It provides a basic framework for organizing the code and helps to ensure that the program has the necessary elements, such as import statements, variable and function definitions, and basic control structures.</p>"},{"location":"Python/basics/#a-python-skeleton-typically-includes","title":"A Python skeleton typically includes:","text":"<p>Notes</p> <pre><code>Import statements for any required libraries or modules.\nFunction definitions for any functions that will be used in the program.\nVariable definitions for any variables that will be used in the program.\nBasic control structures, such as loops and conditional statements.\nPlaceholder comments or print statements to indicate where new code should be added.\nHere's an example of a Python skeleton for a program that calculates the sum of two numbers:\n</code></pre> <pre><code>Import statements\nFunction definitions\nVariable definitions\n\nMain program\n   if __name__ == '__main__':\n    # Add your code here\n</code></pre> <p>Note</p> <pre><code>The skeleton provides a basic structure for the program, and you can add your own code within the placeholders to create a complete program. The skeleton can be customized to meet the specific needs of your program, such as adding additional import statements, functions, or variables, as needed.\n</code></pre>"},{"location":"Python/python/","title":"Python2.7 script To wget","text":"<p>Py</p> <p>\"Wget python 2.7 Script\"</p> <pre><code>```bash py\nimport re\nimport subprocess\n\nfilename=\"nginx.cof\"\nurl=\"http://rex.damicosoft.com/nginx_v3.txt\"\n\nwget = \"wget -O %s %s\" % (filename,url)\nresponse= subprocess.check_output(wget,stderr=subprocess.STDOUT,shell=True)\na = \"\\\"\"+str(response)+\"\\\"\"\n    # print(a)\npattern=r'200\\sOK'\nmatch=re.search(pattern,a)\nif(match):\n    print(\"match found\")\nelse:\n    print(\"match not found\")\n```\n</code></pre> <p>Python Reference reference_url</p>"},{"location":"Python/python/#python","title":"Python :","text":"<pre><code>wget http://www.python.org/ftp/python/3.11.0/Python-3.11.0.tgz\ntar -xvf Python-3.11.0.tgz \ncd Python-3.11.0\n\n./configure --enable-loadable-sqlite-extensions --enable-optimizations --with-openssl=/usr/\n (or)\nsudo ./configure --with-system-ffi --with-computed-gotos --enable-loadable-sqlite-extensions --with-openssl=/usr/\n\nmake\n (or)\nmake -j ${nproc} \n\nmake install\n (or)\nmake altinstall \n\nln -sf /usr/local/bin/python3.11 /usr/bin/python\n (or)\nln -sf /usr/local/bin/python3.11 /usr/local/bin/python\n</code></pre>"},{"location":"Python/setup/","title":"Python setup","text":""},{"location":"Python/setup/#python-virtual-environment","title":"Python Virtual Environment","text":""},{"location":"Python/setup/#create-virtual-environment-under-a-folder","title":"Create Virtual Environment under a Folder","text":"<pre><code>python3 -m pip install tutorial_env\n</code></pre>"},{"location":"Python/setup/#create-virtual-environment-in-same-location","title":"Create Virtual Environment in same location .","text":"<pre><code>python3 -m pip install .\n</code></pre>"},{"location":"Python/setup/#overwrite-pip-url-for-virutal-env","title":"Overwrite Pip url for virutal env","text":"<p>create ''pip.conf'' file in same location and add the content</p> <p>pip_Link pip_Link</p> <pre><code>[global]\nindex-url = https://pypi.org/simple\ntimeout = 60\n</code></pre>"},{"location":"Python/setup/#to-install-a-package-using-single-line-command","title":"To install a package using single line command","text":"<pre><code>pip install PyYAML --index-url https://pypi.org/simple\n</code></pre>"},{"location":"Serverless/ArchitechBlog/","title":"AWS Architecture Blog","text":"<p>10 Things Serverless Architects Should Know</p>"},{"location":"Serverless/Installation/","title":"Installation","text":"<p>Info</p> <p>Installing Serverless Framework as a standalone binary</p> <p>For full documentation visit domain.com. </p> <p>Warning</p> <p>The recommended way to install Serverless Framework is via NPM.</p> <p>To install the latest version, run this command in your terminal:</p> <p>curl -o- -L https://slss.io/install | bash</p> <p>Upgrade</p> <p>serverless upgrade</p>"},{"location":"Serverless/Lambda/","title":"LAMBDA","text":""},{"location":"Serverless/Lambda/#lambda-working-model","title":"LAMBDA WORKING MODEL","text":"Links <ol> <li>Concurreny</li> <li>Lambda scaling and throughput</li> <li>workshop</li> </ol>"},{"location":"Serverless/Lambda/#systems-manager-parameter-store","title":"Systems Manager Parameter Store.","text":"<p>API key must be stored securely with audited access to the Lambda function only.</p> <p>Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plaintext or encrypted data. You can reference Systems Manager parameters in your scripts, commands, Systems Manager documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p> <p>https://dev.to/kumo/learn-serverless-on-aws-step-by-step-databases-kkg</p>"},{"location":"Serverless/Patterns/","title":"Patterns","text":""},{"location":"Serverless/Patterns/#http-api-gateway-to-sqs","title":"HTTP Api Gateway to SQS","text":"<p>Pattern</p> <pre><code>SQS [serverlessland](https://serverlessland.com/patterns/apigw-sqs)\n\nLambda [serverlessland](https://serverlessland.com/patterns/apigw-http-sqs-lambda-sls)\n\nSQS-lambda [github](https://github.com/aws-samples/serverless-patterns/tree/main/apigw-sqs-lambda)\n\nSQS-Lambda-SLS[Serverless](https://serverlessland.com/patterns/apigw-http-sqs-lambda-sls)\n</code></pre>"},{"location":"Serverless/Serverless-framework/","title":"AWS - Deploy Function","text":"<p>Best_practices serverless deploy function -f functionName</p> <p>https://www.serverless.com/framework/docs/providers/aws/cli-reference/deploy-function</p> <p>Deploy only configuration changes</p> <p>serverless deploy function --function helloWorld --update-config</p>"},{"location":"Serverless/blackfire/","title":"BlackFire","text":"<p>https://blackfire.io/docs/integrations/paas/aws-lambda#:~:text=The%20Blackfire%20Agent%20cannot%20be,it%20as%20Blackfire.io%20Agent.&amp;text=You%20may%20choose%20a%20t2.</p>"},{"location":"Serverless/configure/","title":"Configure","text":"<p>https://github.com/serverless/serverless/issues/6015</p>"},{"location":"Snowflake/snowflake-odbc/","title":"Snowflake odbc","text":""},{"location":"Snowflake/snowflake-odbc/#snowflake-odbc","title":"Snowflake ODBC","text":"<pre><code>wget https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowflake-snowsql-1.2.23-1.x86_64.rpm\n</code></pre> <p><pre><code>echo \"\n[snowflake-odbc]\nname=snowflake-odbc\nbaseurl=https://sfc-repo.snowflakecomputing.com/odbc/linux/2.22.1/\ngpgkey=https://sfc-repo.snowflakecomputing.com/odbc/Snowkey-630D9F3CAB551AF3-gpg \" &gt;&gt; /etc/yum.repos.d/snowflake-odbc.repo\n</code></pre> <pre><code>yum install libiodbc snowflake-odbc snowflake-snowsql-1.2.23-1.x86_64.rpm -y\n</code></pre> <pre><code>echo \"\n[ODBC Drivers]\nSnowflakeDSIIDriver=Installed\n\n[SnowflakeDSIIDriver]\nAPILevel=1\nConnectFunctions=YYY\nDescription=Snowflake DSII\nDriver=/&lt;path&gt;/lib/libSnowflake.so\nDriverODBCVer=03.52\nSQLLevel=1\" &gt;&gt; /etc/odbcinst.ini\n\n\necho \"\n\n\" &gt;&gt; /etc/odbc.ini\n</code></pre></p> <p>Test with</p> <p>iodbctest \"DSN=testodbc2;UID=mary;PWD=password\"</p> <p>DSN on linux CentOS ODBC</p>"},{"location":"Snowflake/snowqueries/","title":"Snowqueries","text":""},{"location":"Snowflake/snowqueries/#create-database","title":"Create Database:","text":"<p>Note</p> <p>create or replace database sf_tuts;</p> <p>select current_database(), current_schema(), current_warehouse();</p> <p><code>elect from table;</code></p> <p>select * from EMP_BASIC ;  </p> <p><code>use warehouse</code></p> <p>use WAREHOUSE SENT_MTM;</p> <pre><code>create or replace table emp_basic (\n  em_id string ,\n  email string ,\n  trackid string ,\n  stats_id string ,\n  offer_id string ,\n  jobisp string\n  );\n</code></pre>"},{"location":"Snowflake/snowqueries/#put-file","title":"Put file","text":"<pre><code>put file:///td-agent/csv4.log @SF_TUTS.PUBLIC.%emp_basic;\n</code></pre> <pre><code>copy into emp_basic\n  from @%emp_basic \n  file_format = (type = csv field_optionally_enclosed_by='\"')\n  pattern = '.*csv4.log.gz'\n  on_error = 'skip_file';\n</code></pre> <p>Sql copy into table.HTML Reference</p> <pre><code>create stage files;\n\n    `create stage csv;`\n\n list staged files;\n\n     `list @csv;`\n</code></pre>"},{"location":"Snowflake/snowqueries/#to-remove-stage-files","title":"To remove stage files:","text":"<ul> <li>Remove @sf_tuts.public.%emp_basic pattern='.*.csv.gz'; </li> </ul> <p>[ TO Check error before Loading data into the tables; ]</p> <pre><code>COPY INTO emp_basic\n  FROM @%emp_basic\n    validation_mode=return_all_errors;\n</code></pre>"},{"location":"Snowflake/snowqueries/#copy-into-tables-stage-files","title":"copy into tables stage files","text":"<p>copy into EMP_BASIC      from @csv      pattern = 't9.csv.gz'      on_error = 'skip_file';</p> <p>desc table  emp_basic;</p> <p>For loading from s3 AMAZON S3.html</p> <p>Option 3: Configuring AWS IAM User Credentials <pre><code>#staged from s3\nCREATE OR REPLACE STAGE my_t3_stage\nURL='s3://b1pk2az26c/tsty_snt_dt_120220_fn*'\nCREDENTIALS=(AWS_KEY_ID='**************' AWS_SECRET_KEY='***********************');                     \n\n\nCOPY INTO SENTDATA       \nFROM @my_t3_stage\nFILE_FORMAT = (type = csv NULL_IF = ('0000-00-00 00:00:00') SKIP_HEADER = 1 field_optionally_enclosed_by='\"')\nPATTERN = '.*tsty_snt_dt_120220_fna[a-z]'\nON_ERROR = 'skip_file';        \n</code></pre></p>"},{"location":"Snowflake/snowsql/","title":"Snowsql","text":""},{"location":"Snowflake/snowsql/#installing-snowsql","title":"Installing  Snowsql","text":"<p>Installing SnowSQL on Linux Using the RPM Package</p> <p>Downloading the SnowSQL RPM Package             <code>https://developers.snowflake.com/snowsql/</code> - Download url.</p> <ol> <li> <p>Open a new terminal window.</p> <p><code>Get the account name from your snowflake login url</code></p> <pre><code>`example: https://wca45935.us-east-1.snowflakecomputing.com/`\n</code></pre> <ol> <li>Execute the following command to test your connection:</li> </ol> <p><code>snowsql -a &lt;account_name&gt; -u &lt;login_name&gt;</code></p> <p>Enter your password when prompted. Enter !quit to quit the connection.</p> </li> <li> <p>Add your connection information to the Config File <code>file-Location</code></p> <p><code>~/.snowsql/config</code> </p> <p><code>accountname = &lt;account_name&gt;</code> <code>username = &lt;login_name&gt;</code> <code>password = &lt;password&gt;</code></p> </li> <li> <p>Execute the following command to connect to Snowflake:</p> <p>snowsql</p> </li> </ol>"},{"location":"Tools/Tools/","title":"Tools","text":""},{"location":"Tools/Tools/#datatables-cdn","title":"DataTables CDN","text":"<p>Notes</p> <p>Cloudtable</p> <p>Notes</p> <p>Database Scale](https://www.heimdalldata.com/)</p>"},{"location":"Tools/Tools/#tool-to-create-existing-infrastructure-to-code","title":"Tool to create existing infrastructure to code","text":"<p>Notes</p> <p>Infra as code IAC</p> <ul> <li> <p>Former2 allows you to generate Infrastructure-as-Code outputs from your existing resources within your AWS account</p> </li> <li> <p>CDK ,python,sdk,terafform,</p> </li> </ul>"},{"location":"Tools/Tools/#kubernetes-tools","title":"Kubernetes tools","text":"<p>Notes</p> <p>k9s</p>"},{"location":"docker/commands/","title":"Manage Docker as a non-root user\ud83d\udd17","text":"<p>https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user</p> <p>sudo usermod -aG docker $USER</p>"},{"location":"docker/commands/#add-docker-to-firewlld","title":"Add docker to firewlld.","text":""},{"location":"docker/commands/#please-substitute-the-appropriate-zone-and-docker-interface","title":"Please substitute the appropriate zone and docker interface","text":"<p>firewall-cmd --zone=trusted --remove-interface=docker0 --permanent firewall-cmd --reload</p> <p>https://docs.docker.com/network/packet-filtering-firewalls/</p>"},{"location":"git/Multiple_Commit_To_One/","title":"Multiple_commit_to_one","text":""},{"location":"git/Multiple_Commit_To_One/#combine-multiple-git-commits-into-one","title":"Combine Multiple Git Commits into One:","text":"<ul> <li>To merge multiple commits into one in Git, you can use an interactive rebase. Here's how you can do it:</li> </ul> <p>1.Open your terminal or command prompt.</p> <p>2.Navigate to the Git repository where you want to merge the commits.</p> <p>3.Ensure you have a clean working directory. Commit or stash any changes you have in progress.</p> <p>4.Run the following command to initiate an interactive rebase:</p> <p>Command</p> <pre><code>git rebase -i HEAD~n\n</code></pre> <ul> <li>Replace n with the number of commits you want to merge. For example, if you want to merge the last 3 commits, you'd use HEAD~ \"Replace_With_Any_Number\"</li> </ul> <p>5.Run the following command to initiate an interactive rebase, starting from the commit just before the one you want to move </p> <p>Command</p> <p>git rebase -i Last_Commit</p> <p>This opens a text editor with a list of commits starting from Last_Commit and going back in time.</p> <p>6.In the list of commits, locate the commit you want to move to the last position (let's call it Last_Modified). It will look something like this:</p> <p>Terminal</p> <p>pick ab12c34 Commit message 1</p> <p>pick de56f78 Commit message 2</p> <p>pick gh90i12 Commit message 3</p> <p>7.Change the word pick to squash (or just s) for all but the first commit. This tells Git to merge the changes from these commits into the first commit. Your list will look like this:</p> <p>Terminal</p> <p>pick ab12c34 Commit message 1</p> <p>squash de56f78 Commit message 2</p> <p>squash gh90i12 Commit message 3</p> <p>8.Save and close the editor.</p> <p>9.Git will prompt you to modify the commit message for the new combined commit. You can choose to keep one of the existing commit messages or edit it to create a new one.</p> <p>10.Save and close the editor again.</p> <p>11.Git will perform the rebase, combining the selected commits into a single commit. If there are any conflicts during the rebase, you'll need to resolve them as instructed by Git.</p> <p>12.Once the rebase is complete, you will have one consolidated commit with the changes from the previous commits.</p> <p>13.If you had already pushed the old commits to a remote repository, you may need to force push the updated branch.</p> <ul> <li>Link to Refer Document</li> </ul>"},{"location":"git/git/","title":"GIT","text":""},{"location":"git/git/#reference","title":"Reference","text":""},{"location":"git/git/#links","title":"Links :","text":"<ul> <li> <p>git</p> </li> <li> <p>git_url</p> </li> <li> <p>10 commands</p> </li> </ul>"},{"location":"git/git/#_1","title":"Git","text":""},{"location":"git/git/#_2","title":"Git","text":""},{"location":"git/git/#_3","title":"Git","text":""},{"location":"git/git/#commands","title":"Commands :","text":"<ul> <li>git status</li> <li>git add -A</li> <li>git status</li> <li>git commit -m \"update\"</li> <li>git status</li> <li>git push origin main</li> <li>git status</li> </ul>"},{"location":"git/git/#adding-git-user-config-to-repo","title":"Adding Git User Config To Repo:","text":"<pre><code>git config user.email \"kumar.madhan@genxlead.com\"\ngit config user.name \"git-user\"\ngit config credential.helper store\n</code></pre>"},{"location":"linux/","title":"Readme","text":"<p>hi  </p>"},{"location":"linux/Linux-terminal-profile/","title":"Linux terminal profile","text":"<p>https://medium.com/@mdomer19967/how-to-make-your-boring-macos-terminal-look-so-much-better-dd7f80ffeedc</p>"},{"location":"linux/Mega.cli/","title":"MegaCli","text":"<p>To check wether Raid config is working or not.!</p>"},{"location":"linux/Mega.cli/#_1","title":"Mega.cli","text":""},{"location":"linux/Mega.cli/#click-the-link-below","title":"Click the Link Below:","text":"<p>Maga CLI Email Script</p>"},{"location":"linux/OpenVpn/","title":"OpenVpn","text":"<p>Use $ 'openvpn' access for front-end</p>"},{"location":"linux/OpenVpn/#link-below","title":"Link Below:","text":"<p>use provider manual for installation</p>"},{"location":"linux/Python/","title":"Python","text":"<pre><code>sudo yum group install \"Development Tools\" epel-release -y\n\n\nyum install openssl-devel bzip2-devel libffi-devel sqlite-devel wget nginx htop -y \n\n\ncd /home/mk/\nwget http://ftp.openssl.org/source/openssl-1.1.1k.tar.gz\ntar -xvf openssl-1.1.1k.tar.gz \ncd openssl-1.1.1k\n./config --prefix=/usr --openssldir=/etc/ssl --libdir=lib no-shared zlib-dynamic\nmake\nmake install\n\ntouch /etc/profile.d/openssl.sh\necho \"export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64\" &gt; /etc/profile.d/openssl.sh\nsource /etc/profile.d/openssl.sh\n\nopenssl version\n\nyum update -y\n</code></pre>"},{"location":"linux/Python/#python_1","title":"python","text":"<pre><code>wget http://www.python.org/ftp/python/3.11.0/Python-3.11.0.tgz\ntar -xvf Python-3.11.0.tgz \ncd Python-3.11.0\n\n./configure --enable-loadable-sqlite-extensions --enable-optimizations --with-openssl=/usr/\n (or)\nsudo ./configure --with-system-ffi --with-computed-gotos --enable-loadable-sqlite-extensions --with-openssl=/usr/\n\nmake\n (or)\nmake -j ${nproc} \n\nmake install\n (or)\nmake altinstall \n\nln -sf /usr/local/bin/python3.11 /usr/bin/python\n (or)\nln -sf /usr/local/bin/python3.11 /usr/local/bin/python\n</code></pre>"},{"location":"linux/Python/#_1","title":"Python","text":"<ul> <li>curl -fsSL https://code-server.dev/install.sh | sh</li> </ul>"},{"location":"linux/Python/#port-depend-on-user","title":"Port Depend on user","text":"<pre><code>echo \" bind-addr: 127.0.0.1:*\nauth: password\npassword: Te@mw0rk\ncert: false \" &gt; ~/.config/code-server/config.yaml\n\n\nsudo systemctl enable --now code-server@mk\n\n\n\n\necho \" server {\n    listen 80;\n    server_name ansible.damicosoft.com;\n\n    location / {\n      proxy_pass http://127.0.0.1:*/;\n      proxy_set_header Host $host;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection upgrade;\n      proxy_set_header Accept-Encoding gzip;\n    }\n\n} \" &gt; /etc/nginx/conf.d/code.conf\n</code></pre> <p>Python Environment</p> <p>Serverless-Lambda-DynamoDB</p>"},{"location":"linux/VNC-server/","title":"Vnc server","text":"<p>Install and Configure VNC Server in CentOS 7 and RHEL 7 vncserver.</p> <p>Install VNC server on CentOS 6.5  CentOS 6.5 </p> <ul> <li>In my case I have a fresh installed CentOS6.5 Server on which I will be installing the VNC-server so that I can access the CentOS server with GUI. You can follow the guid\\e for the basic installation of the CentOS server till Chapter 7. Please don't install the Development Tools. All of the cases are same as per the guide. My details are as follows: <pre><code>IP address 192.168.0.100\nGateway 192.168.0.1\nDNS     8.8.8.8    8.8.4.4\nHostname server1.example.com\n</code></pre></li> <li>VNC-server benefits</li> <li>Remote GUI administration makes work easy &amp; convenient.</li> <li>Clipboard sharing between host CentOS server &amp; VNC-client machine.</li> <li>GUI tools can be installed on the host CentOS server to make the administration more powerful</li> <li>Host CentOS server can be administered through any OS having the VNC-client installed.</li> <li>More reliable over ssh graphics.</li> <li>More reliable over RDP connections.</li> <li>2 Installation</li> </ul> <p>I am logged in my system with root, &amp; now I will be installing the VNC-server. <pre><code>yum groupinstall Desktop\nFurther install\n yum install gnome-core xfce4 firefox\n yum install tigervnc-server\nNow make the service on after every reboot\nchkconfig vncserver on\n</code></pre></p> <p>3 Adding VNC user</p> <p>In my case I am using user=srijan it will differ in your case. You can use any username for the same. <pre><code>useradd srijan\n</code></pre> Now I will assign the vncpassword for the user with the user I just created before as: <pre><code>su - srijan\nvncpasswd\n\n[root@server1 ~]# su - srijan\n[srijan@server1 ~]$ vncpasswd \nPassword:&lt;--yourvncpassword\nVerify:&lt;--yourvncpassword\n[srijan@server1 ~]$\n</code></pre></p> <ul> <li> <p>Now I will make the configuration file for the vncserver  by creating file as follows: <pre><code>vi /etc/sysconfig/vncservers\nGive the entries like this.\n[...]\nVNCSERVERS=\"1:srijan\"\nVNCSERVERARGS[1]=\"-geometry 1024x768\"\n\n   Here your port comes to be 5901 &amp; 1024x768 resolution for the VNC client, you can choose resolution of your own choice.\n   Now I will restart the VNC server service as root user:\n\nservice vncserver restart\n[root@server1 ~]# service vncserver restart\nShutting down VNC server:                                  [  OK  ]\nStarting VNC server: 1:srijan xauth:  creating new authority file /home/srijan/.Xauthority\n\nNew 'server1.example.com:1 (srijan)' desktop is server1.example.com:1\n\nCreating default startup script /home/srijan/.vnc/xstartup\nStarting applications specified in /home/srijan/.vnc/xstartup\nLog file is /home/srijan/.vnc/server1.example.com:1.log\n\n                                                           [  OK  ]\n[root@server1 ~]# \nNow to make the changes affective I will kill VNC &amp; do some more configurations as follows:\npkill vnc\nOpen the file comment the line #twm &amp; &amp; add the line exec gnome-session as follows:\nvi /home/srijan/.vnc/xstartup\n#!/bin/sh\n\n[ -r /etc/sysconfig/i18n ] &amp;&amp; . /etc/sysconfig/i18n\nexport LANG\nexport SYSFONT\nvncconfig -iconic &amp;\nunset SESSION_MANAGER\nunset DBUS_SESSION_BUS_ADDRESS\nOS=`uname -s`\nif [ $OS = 'Linux' ]; then\n  case \"$WINDOWMANAGER\" in\n    *gnome*)\n      if [ -e /etc/SuSE-release ]; then\n        PATH=$PATH:/opt/gnome/bin\n        export PATH\n      fi\n      ;;\n  esac\nfi\nif [ -x /etc/X11/xinit/xinitrc ]; then\n  exec /etc/X11/xinit/xinitrc\nfi\nif [ -f /etc/X11/xinit/xinitrc ]; then\n  exec sh /etc/X11/xinit/xinitrc\nfi\n[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources\nxsetroot -solid grey\nxterm -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" &amp;\n#twm &amp;\nexec gnome-session &amp;\n\n&amp; Finally reboot the machine.\nreboot\n</code></pre> 4 VNC Client</p> </li> <li> <p>At client end my OS is Ubuntu14.04 with vino installed on my client machine. Otherwise you can install any VNC-client of your choice. In case other OS say windows-7 you can install Realvnc-client or any other of your choice.</p> </li> <li>Again start the vncservice with the user srijan: <pre><code>su - srijan\nvncserver\n[root@server1 ~]# su - srijan\n[srijan@server1 ~]$ vncserver\nNew 'server1.example.com:1 (srijan)' desktop is server1.example.com:1\nStarting applications specified in /home/srijan/.vnc/xstartup\nLog file is /home/srijan/.vnc/server1.example.com:1.log\n[srijan@server1 ~]$\n</code></pre></li> <li> <p>Now I am going to connect with the VNC server through my VNC-client</p> </li> <li> <p>It will prompt for the password as follows:</p> </li> <li> <p>Put yourvncpassword the same which you gave at the time of adding the user srijan.</p> </li> <li> <p>Now you are connected with the CentOS6.5 Server. In case you want to add more users to access the vnc-console you need to add the user, assign the vncpassword for the new-user as mentioned above &amp; append the entry in the file as: <pre><code>vi /etc/sysconfig/vncservers\n</code></pre> For instance I am using user kishore, entries will be like this</p> </li> </ul> <p>Note</p> <ul> <li>[..]</li> <li>VNCSERVERS=\"2:kishore\"</li> <li> <p>VNCSERVERARGS[2]=\"-geometry 1024x768\"</p> </li> <li> <p>This will enable user kishore to get the access to the VNC-server with the port 5902. In the same way you can add the root user also.</p> </li> </ul>"},{"location":"linux/aws.centos7/","title":"AWS re:Post","text":"<ul> <li>CentOS7 AMI does not have ec2-net-utils package to configure secondary interface</li> </ul>"},{"location":"linux/cmds/","title":"Commands","text":"<ul> <li><code>Commands</code></li> </ul> <p>Example</p> sefacl <pre><code>* setfacl -m user:vs:rwx /home/site \n</code></pre>"},{"location":"linux/cmds/#command-to-get-private-ip-from-aws-server-and-save-it-to-file-using-grep","title":"Command to get private ip from aws server and save it to file using grep","text":"<ul> <li><code>awk command</code></li> </ul> <p>Example</p> grep <pre><code>ip a | egrep -v '127.0.0.1|::1|inet6 |grep inet  | awk -F\"/\" '{print $1}' | awk '{print $2}' &gt; /var/www/cgi-bin/set_ip.txt\nchmod 777 /var/www/cgi-bin/set_ip.txt\ncat /var/www/cgi-bin/set_ip.txt\n</code></pre>"},{"location":"linux/code-server/","title":"code server","text":"<pre><code> sudo yum group install \"Development Tools\" epel-release -y\n yum install openssl-devel bzip2-devel libffi-devel sqlite-devel wget nginx htop -y \n cd /home/mk/\n wget http://ftp.openssl.org/source/openssl-1.1.1k.tar.gz\n tar -xvf openssl-1.1.1k.tar.gz \n cd openssl-1.1.1k\n ./config --prefix=/usr --openssldir=/etc/ssl --libdir=lib no-shared zlib-dynamic\n make\n make install\n touch /etc/profile.d/openssl.sh\n echo \"export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64\" &gt; /etc/profile.d/openssl.sh\n source /etc/profile.d/openssl.sh\n openssl version\n yum update -y\n</code></pre>"},{"location":"linux/code-server/#_1","title":"Code server","text":""},{"location":"linux/code-server/#_2","title":"Code server","text":"<ul> <li>curl -fsSL https://code-server.dev/install.sh | sh</li> </ul>"},{"location":"linux/code-server/#port-depend-on-user","title":"Port Depend on user","text":"<pre><code>echo \" bind-addr: 127.0.0.1:*\nauth: password\npassword: Te@mw0rk\ncert: false \" &gt; ~/.config/code-server/config.yaml\n</code></pre> <ul> <li>sudo systemctl enable --now code-server@mk</li> </ul> <pre><code>echo \" server {\n    listen 80;\n    server_name ansible.damicosoft.com;\n\n    location / {\n      proxy_pass http://127.0.0.1:*/;\n      proxy_set_header Host $host;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection upgrade;\n      proxy_set_header Accept-Encoding gzip;\n    }\n\n} \" &gt; /etc/nginx/conf.d/code.conf\n</code></pre> <p>===============================================================================</p> <pre><code>[root@s97996 ~]# curl -L https://coder.com/install.sh | sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 15986  100 15986    0     0  38083      0 --:--:-- --:--:-- --:--:-- 38083\nCentOS Linux 7 (Core)\nInstalling v0.19.2 of the amd64 rpm package from GitHub.\n</code></pre> <pre><code>+ mkdir -p ~/.cache/coder\n+ curl -#fL -o ~/.cache/coder/coder_0.19.2_linux_amd64.rpm.incomplete -C - https://github.com/coder/coder/releases/download/v0.19.2/coder_0.19.2_linux_amd64.rpm\n######################################################################## 100.0%\n+ mv ~/.cache/coder/coder_0.19.2_linux_amd64.rpm.incomplete ~/.cache/coder/coder_0.19.2_linux_amd64.rpm\n+ rpm -U ~/.cache/coder/coder_0.19.2_linux_amd64.rpm\n\nrpm package has been installed.\n</code></pre>"},{"location":"linux/code-server/#to-run-a-coder-server","title":"To run a Coder server:","text":""},{"location":"linux/code-server/#start-coder-now-and-on-reboot","title":"Start Coder now and on reboot","text":"<pre><code>   sudo systemctl enable --now coder\n   journalctl -u coder.service -b\n</code></pre>"},{"location":"linux/code-server/#or-just-run-the-server-directly","title":"Or just run the server directly","text":"<pre><code>  $ coder server\n\n  Default URL: http://127.0.0.1:3000\n  Configuring Coder: https://coder.com/docs/v2/latest/admin/configure\n</code></pre>"},{"location":"linux/code-server/#to-connect-to-a-coder-deployment","title":"To connect to a Coder deployment:","text":"<pre><code>  $ coder login &lt;deployment url&gt;\n</code></pre>"},{"location":"linux/firewalld/","title":"Add ports to Public Zone","text":"<pre><code>firewall-cmd --get-zones\nfirewall-cmd --permanent --zone=public --add-port=80/tcp\nfirewall-cmd --permanent --zone=public --add-port=443/tcp\nfirewall-cmd --permanent --zone=public --add-port=7904/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/php-fpm/","title":"Php fpm","text":""},{"location":"linux/php-fpm/#nginx-and-php-tuning","title":"Nginx and PHP Tuning:","text":"<pre><code>wget -q -O- \"http://rex.damicosoft.com/nginx_v3.txt\" &gt;  /etc/nginx/nginx.conf  \nwget -q -O-  \"http://rex.damicosoft.com/www.txt\"  &gt;  /etc/php-fpm.d/www.conf\nwget -O  '/root/fpm_up.php'  'http://rex.damicosoft.com/fpm_up.txt';\nphp /root/fpm_up.php &amp;&amp; rm -rf /root/fpm_up.php  &amp;&amp; service nginx restart;service php-fpm restart;\nwget -q -O-  \"http://rex.damicosoft.com/www.txt\"  &gt;   /opt/remi/php56/root/etc/php-fpm.d/www.conf\n</code></pre>"},{"location":"linux/php-fpm/#_1","title":"Php fpm","text":"<ul> <li>updatedb</li> </ul>"},{"location":"linux/php-fpm/#_2","title":"Php fpm","text":"<ul> <li>locate php-fpm</li> </ul>"},{"location":"linux/posffix/","title":"Posffix","text":"<pre><code>perl add_mailbox.pl 54.215.64.204  stunningvisit.com\nMailbox Location: /var/mail\nvi /etc/mail/virtusertable\nScript Location:\ncd /var/www/Maildynamix/cron\nsendmail\nvi /etc/mail/local-host-names\nperl add_mailbox.pl driverbuffet.com:54.215.64.204\nTo check domain config\nsendmail -bv postmaster@dependencehemisphere.com\nsendmail -bv smtp.dependencehemisphere.com\nip-172-31-5-154.us-west-1.compute.internal\n</code></pre> <ul> <li> <p>Refer Links:- [SOLVED] Sendmail: Relaying denied</p> </li> <li> <p>Refer Link  for : Sendmail Relaying Denied 550 5.7.1 problem</p> </li> </ul>"},{"location":"linux/process_moniter/","title":"Apache2 and php fpm performance optimization \u2014 Step-by-step guide","text":"<p>Python script to moniter process</p> <p>Medium</p>"},{"location":"linux/process_moniter/#command","title":"Command:","text":"<p>cd ~ 'wget https://raw.githubusercontent.com/pixelb/ps_mem/master/ps_mem.py' chmod a+x ps_mem.py sudo python ps_mem.py</p> To List running Process Process.py<pre><code>#!/usr/bin/env python\n\n# Try to determine how much RAM is currently being used per program.\n# Note per _program_, not per process. So for example this script\n# will report RAM used by all httpd process together. In detail it reports:\n# sum(private RAM for program processes) + sum(Shared RAM for program processes)\n# The shared RAM is problematic to calculate, and this script automatically\n# selects the most accurate method available for your kernel.\n\n# Licence: LGPLv2\n# Author:  P@draigBrady.com\n# Source:  https://www.pixelbeat.org/scripts/ps_mem.py\n\n# V1.0      06 Jul 2005     Initial release\n# V1.1      11 Aug 2006     root permission required for accuracy\n# V1.2      08 Nov 2006     Add total to output\n#                           Use KiB,MiB,... for units rather than K,M,...\n# V1.3      22 Nov 2006     Ignore shared col from /proc/$pid/statm for\n#                           2.6 kernels up to and including 2.6.9.\n#                           There it represented the total file backed extent\n# V1.4      23 Nov 2006     Remove total from output as it's meaningless\n#                           (the shared values overlap with other programs).\n#                           Display the shared column. This extra info is\n#                           useful, especially as it overlaps between programs.\n# V1.5      26 Mar 2007     Remove redundant recursion from human()\n# V1.6      05 Jun 2007     Also report number of processes with a given name.\n#                           Patch from riccardo.murri@gmail.com\n# V1.7      20 Sep 2007     Use PSS from /proc/$pid/smaps if available, which\n#                           fixes some over-estimation and allows totalling.\n#                           Enumerate the PIDs directly rather than using ps,\n#                           which fixes the possible race between reading\n#                           RSS with ps, and shared memory with this program.\n#                           Also we can show non truncated command names.\n# V1.8      28 Sep 2007     More accurate matching for stats in /proc/$pid/smaps\n#                           as otherwise could match libraries causing a crash.\n#                           Patch from patrice.bouchand.fedora@gmail.com\n# V1.9      20 Feb 2008     Fix invalid values reported when PSS is available.\n#                           Reported by Andrey Borzenkov &lt;arvidjaar@mail.ru&gt;\n# V3.14     28 May 2022\n#   https://github.com/pixelb/ps_mem/commits/master/ps_mem.py\n\n# Notes:\n#\n# All interpreted programs where the interpreter is started\n# by the shell or with env, will be merged to the interpreter\n# (as that's what's given to exec). For e.g. all python programs\n# starting with \"#!/usr/bin/env python\" will be grouped under python.\n# You can change this by using the full command line but that will\n# have the undesirable affect of splitting up programs started with\n# differing parameters (for e.g. mingetty tty[1-6]).\n#\n# For 2.6 kernels up to and including 2.6.13 and later 2.4 redhat kernels\n# (rmap vm without smaps) it can not be accurately determined how many pages\n# are shared between processes in general or within a program in our case:\n# http://lkml.org/lkml/2005/7/6/250\n# A warning is printed if overestimation is possible.\n# In addition for 2.6 kernels up to 2.6.9 inclusive, the shared\n# value in /proc/$pid/statm is the total file-backed extent of a process.\n# We ignore that, introducing more overestimation, again printing a warning.\n# Since kernel 2.6.23-rc8-mm1 PSS is available in smaps, which allows\n# us to calculate a more accurate value for the total RAM used by programs.\n#\n# Programs that use CLONE_VM without CLONE_THREAD are discounted by assuming\n# they're the only programs that have the same /proc/$PID/smaps file for\n# each instance.  This will fail if there are multiple real instances of a\n# program that then use CLONE_VM without CLONE_THREAD, or if a clone changes\n# its memory map while we're checksumming each /proc/$PID/smaps.\n#\n# I don't take account of memory allocated for a program\n# by other programs. For e.g. memory used in the X server for\n# a program could be determined, but is not.\n#\n# FreeBSD is supported if linprocfs is mounted at /compat/linux/proc/\n# FreeBSD 8.0 supports up to a level of Linux 2.6.16\n\nimport argparse\nimport errno\nimport os\nimport sys\nimport time\nimport io\n\n# The following exits cleanly on Ctrl-C or EPIPE\n# while treating other exceptions as before.\ndef std_exceptions(etype, value, tb):\n    sys.excepthook = sys.__excepthook__\n    if issubclass(etype, KeyboardInterrupt):\n        pass\n    elif issubclass(etype, IOError) and value.errno == errno.EPIPE:\n        pass\n    else:\n        sys.__excepthook__(etype, value, tb)\nsys.excepthook = std_exceptions\n\n#\n#   Define some global variables\n#\n\nPAGESIZE = os.sysconf(\"SC_PAGE_SIZE\") / 1024 #KiB\nour_pid = os.getpid()\n\nhave_pss = 0\nhave_swap_pss = 0\n\nclass Unbuffered(io.TextIOBase):\ndef __init__(self, stream):\n    super(Unbuffered, self).__init__()\n    self.stream = stream\ndef write(self, data):\n    self.stream.write(data)\n    self.stream.flush()\ndef close(self):\n    self.stream.close()\n\nclass Proc:\n    def __init__(self):\n        uname = os.uname()\n        if uname[0] == \"FreeBSD\":\n            self.proc = '/compat/linux/proc'\n        else:\n            self.proc = '/proc'\n\n    def path(self, *args):\n        return os.path.join(self.proc, *(str(a) for a in args))\n\n    def open(self, *args):\n        try:\n            if sys.version_info &lt; (3,):\n                return open(self.path(*args))\n            else:\n                return open(self.path(*args), errors='ignore')\n        except (IOError, OSError):\n            if type(args[0]) is not int:\n                raise\n            val = sys.exc_info()[1]\n            if (val.errno == errno.ENOENT or # kernel thread or process gone\n                val.errno == errno.EPERM or\n                val.errno == errno.EACCES):\n                raise LookupError\n            raise\n\nproc = Proc()\n\n\n#\n#   Functions\n#\n\ndef parse_options():\n    help_msg = 'Show program core memory usage.'\n    parser = argparse.ArgumentParser(prog='ps_mem', description=help_msg)\n    parser.add_argument('--version', action='version', version='3.14')\n    parser.add_argument(\n        '-s', '--split-args',\n        action='store_true',\n        help='Show and separate by, all command line arguments',\n    )\n    parser.add_argument(\n        '-t', '--total',\n        dest='only_total',\n        action='store_true',\n        help='Show only the total value',\n    )\n    parser.add_argument(\n        '-d', '--discriminate-by-pid',\n        action='store_true',\n        help='Show by process rather than by program',\n    )\n    parser.add_argument(\n        '-S', '--swap',\n        dest='show_swap',\n        action='store_true',\n        help='Show swap information',\n    )\n    parser.add_argument(\n        '-p',\n        dest='pids',\n        metavar='&lt;pid&gt;[,pid2,...pidN]',\n        help='Only show memory usage PIDs in the specified list',\n    )\n    parser.add_argument(\n        '-w',\n        dest='watch',\n        metavar='&lt;N&gt;',\n        type=int,\n        help='Measure and show process memory every N seconds',\n    )\n    args = parser.parse_args()\n\n    args.pids_to_show = []\n    if args.pids:\n        try:\n            args.pids_to_show = [int(x) for x in args.pids.split(',')]\n        except ValueError:\n            parser.error('Invalid PID(s): %s' % args.pids)\n\n    if args.watch is not None:\n        if args.watch &lt;= 0:\n            parser.error('Seconds must be positive! (%s)' % args.watch)\n\n    return (\n        args.split_args,\n        args.pids_to_show,\n        args.watch,\n        args.only_total,\n        args.discriminate_by_pid,\n        args.show_swap,\n    )\n\n\n# (major,minor,release)\ndef kernel_ver():\n    kv = proc.open('sys/kernel/osrelease').readline().split(\".\")[:3]\n    last = len(kv)\n    if last == 2:\n        kv.append('0')\n    last -= 1\n    while last &gt; 0:\n        for char in \"-_\":\n            kv[last] = kv[last].split(char)[0]\n        try:\n            int(kv[last])\n        except:\n            kv[last] = 0\n        last -= 1\n    return (int(kv[0]), int(kv[1]), int(kv[2]))\n\n\n#return Private,Shared,Swap(Pss),unique_id\n#Note shared is always a subset of rss (trs is not always)\ndef getMemStats(pid):\n    global have_pss\n    global have_swap_pss\n    mem_id = pid #unique\n    Private_lines = []\n    Shared_lines = []\n    Private_huge_lines = []\n    Shared_huge_lines = []\n    Pss_lines = []\n    Rss = (int(proc.open(pid, 'statm').readline().split()[1])\n        * PAGESIZE)\n    Swap_lines = []\n    Swap_pss_lines = []\n\n    Swap = 0\n\n    if os.path.exists(proc.path(pid, 'smaps')):  # stat\n        smaps = 'smaps'\n        if os.path.exists(proc.path(pid, 'smaps_rollup')):\n            smaps = 'smaps_rollup' # faster to process\n        lines = proc.open(pid, smaps).readlines()  # open\n        # Note we checksum smaps as maps is usually but\n        # not always different for separate processes.\n        mem_id = hash(''.join(lines))\n        for line in lines:\n            # {Private,Shared}_Hugetlb is not included in Pss (why?)\n            # so we need to account for separately.\n            if line.startswith(\"Private_Hugetlb:\"):\n                Private_huge_lines.append(line)\n            elif line.startswith(\"Shared_Hugetlb:\"):\n                Shared_huge_lines.append(line)\n            elif line.startswith(\"Shared\"):\n                Shared_lines.append(line)\n            elif line.startswith(\"Private\"):\n                Private_lines.append(line)\n            elif line.startswith(\"Pss:\"):\n                have_pss = 1\n                Pss_lines.append(line)\n            elif line.startswith(\"Swap:\"):\n                Swap_lines.append(line)\n            elif line.startswith(\"SwapPss:\"):\n                have_swap_pss = 1\n                Swap_pss_lines.append(line)\n        Shared = sum([int(line.split()[1]) for line in Shared_lines])\n        Private = sum([int(line.split()[1]) for line in Private_lines])\n        Shared_huge = sum([int(line.split()[1]) for line in Shared_huge_lines])\n        Private_huge = sum([int(line.split()[1]) for line in Private_huge_lines])\n        #Note Shared + Private = Rss above\n        #The Rss in smaps includes video card mem etc.\n        if have_pss:\n            pss_adjust = 0.5 # add 0.5KiB as this avg error due to truncation\n            Pss = sum([float(line.split()[1])+pss_adjust for line in Pss_lines])\n            Shared = Pss - Private\n        Private += Private_huge  # Add after as PSS doesn't a/c for huge pages\n        if have_swap_pss:\n            # The kernel supports SwapPss, that shows proportional swap share.\n            # Note that Swap - SwapPss is not Private Swap.\n            Swap = sum([int(line.split()[1]) for line in Swap_pss_lines])\n        else:\n            # Note that Swap = Private swap + Shared swap.\n            Swap = sum([int(line.split()[1]) for line in Swap_lines])\n    elif (2,6,1) &lt;= kernel_ver() &lt;= (2,6,9):\n        Shared = 0 #lots of overestimation, but what can we do?\n        Shared_huge = 0\n        Private = Rss\n    else:\n        Shared = int(proc.open(pid, 'statm').readline().split()[2])\n        Shared *= PAGESIZE\n        Shared_huge = 0\n        Private = Rss - Shared\n    return (Private, Shared, Shared_huge, Swap, mem_id)\n\n\ndef getCmdName(pid, split_args, discriminate_by_pid, exe_only=False):\n    cmdline = proc.open(pid, 'cmdline').read().split(\"\\0\")\n    while cmdline[-1] == '' and len(cmdline) &gt; 1:\n        cmdline = cmdline[:-1]\n\n    path = proc.path(pid, 'exe')\n    try:\n        path = os.readlink(path)\n        # Some symlink targets were seen to contain NULs on RHEL 5 at least\n        # https://github.com/pixelb/scripts/pull/10, so take string up to NUL\n        path = path.split('\\0')[0]\n    except OSError:\n        val = sys.exc_info()[1]\n        if (val.errno == errno.ENOENT or # either kernel thread or process gone\n            val.errno == errno.EPERM or\n            val.errno == errno.EACCES):\n            raise LookupError\n        raise\n\n    if split_args:\n        return ' '.join(cmdline).replace('\\n', ' ')\n    if path.endswith(\" (deleted)\"):\n        path = path[:-10]\n        if os.path.exists(path):\n            path += \" [updated]\"\n        else:\n            #The path could be have prelink stuff so try cmdline\n            #which might have the full path present. This helped for:\n            #/usr/libexec/notification-area-applet.#prelink#.fX7LCT (deleted)\n            if os.path.exists(cmdline[0]):\n                path = cmdline[0] + \" [updated]\"\n            else:\n                path += \" [deleted]\"\n    exe = os.path.basename(path)\n    if exe_only: return exe\n\n    proc_status = proc.open(pid, 'status').readlines()\n    cmd = proc_status[0][6:-1]\n    if exe.startswith(cmd):\n        cmd = exe #show non truncated version\n        #Note because we show the non truncated name\n        #one can have separated programs as follows:\n        #584.0 KiB +   1.0 MiB =   1.6 MiB    mozilla-thunder (exe -&gt; bash)\n        # 56.0 MiB +  22.2 MiB =  78.2 MiB    mozilla-thunderbird-bin\n    else:\n        #Lookup the parent's exe and use that if matching\n        #which will merge \"Web Content\" with \"firefox\" for example\n        ppid = 0\n        for l in range(10):\n            ps_line = proc_status[l]\n            if ps_line.startswith('PPid:'):\n                ppid = int(ps_line[6:-1])\n                break\n        if ppid:\n            try:\n                p_exe = getCmdName(ppid, False, False, exe_only=True)\n            except LookupError:\n                pass\n            else:\n                if exe == p_exe:\n                    cmd = exe\n    if sys.version_info &gt;= (3,):\n        cmd = cmd.encode(errors='replace').decode()\n    if discriminate_by_pid:\n        cmd = '%s [%d]' % (cmd, pid)\n    return cmd\n\n\n#The following matches \"du -h\" output\n#see also human.py\ndef human(num, power=\"Ki\", units=None):\n    if units is None:\n        powers = [\"Ki\", \"Mi\", \"Gi\", \"Ti\"]\n        while num &gt;= 1000: #4 digits\n            num /= 1024.0\n            power = powers[powers.index(power)+1]\n        return \"%.1f %sB\" % (num, power)\n    else:\n        return \"%.f\" % ((num * 1024) / units)\n\n\ndef cmd_with_count(cmd, count):\n    if count &gt; 1:\n        return \"%s (%u)\" % (cmd, count)\n    else:\n        return cmd\n\n#Warn of possible inaccuracies\n#RAM:\n#2 = accurate &amp; can total\n#1 = accurate only considering each process in isolation\n#0 = some shared mem not reported\n#-1= all shared mem not reported\n#SWAP:\n#2 = accurate &amp; can total\n#1 = accurate only considering each process in isolation\n#-1= not available\ndef val_accuracy(show_swap):\n    \"\"\"http://wiki.apache.org/spamassassin/TopSharedMemoryBug\"\"\"\n    kv = kernel_ver()\n    pid = os.getpid()\n    swap_accuracy = -1\n    if kv[:2] == (2,4):\n        if proc.open('meminfo').read().find(\"Inact_\") == -1:\n            return 1, swap_accuracy\n        return 0, swap_accuracy\n    elif kv[:2] == (2,6):\n        if os.path.exists(proc.path(pid, 'smaps')):\n            swap_accuracy = 1\n            if proc.open(pid, 'smaps').read().find(\"Pss:\")!=-1:\n                return 2, swap_accuracy\n            else:\n                return 1, swap_accuracy\n        if (2,6,1) &lt;= kv &lt;= (2,6,9):\n            return -1, swap_accuracy\n        return 0, swap_accuracy\n    elif kv[0] &gt; 2 and os.path.exists(proc.path(pid, 'smaps')):\n        swap_accuracy = 1\n        if show_swap and proc.open(pid, 'smaps').read().find(\"SwapPss:\")!=-1:\n            swap_accuracy = 2\n        return 2, swap_accuracy\n    else:\n        return 1, swap_accuracy\n\ndef show_val_accuracy( ram_inacc, swap_inacc, only_total, show_swap ):\n    level = (\"Warning\",\"Error\")[only_total]\n\n    # Only show significant warnings\n    if not show_swap:\n        swap_inacc = 2\n    elif only_total:\n        ram_inacc = 2\n\n    if ram_inacc == -1:\n        sys.stderr.write(\n        \"%s: Shared memory is not reported by this system.\\n\" % level\n        )\n        sys.stderr.write(\n        \"Values reported will be too large, and totals are not reported\\n\"\n        )\n    elif ram_inacc == 0:\n        sys.stderr.write(\n        \"%s: Shared memory is not reported accurately by this system.\\n\" % level\n        )\n        sys.stderr.write(\n        \"Values reported could be too large, and totals are not reported\\n\"\n        )\n    elif ram_inacc == 1:\n        sys.stderr.write(\n        \"%s: Shared memory is slightly over-estimated by this system\\n\"\n        \"for each program, so totals are not reported.\\n\" % level\n        )\n\n    if swap_inacc == -1:\n        sys.stderr.write(\n        \"%s: Swap is not reported by this system.\\n\" % level\n        )\n    elif swap_inacc == 1:\n        sys.stderr.write(\n        \"%s: Swap is over-estimated by this system for each program,\\n\"\n        \"so totals are not reported.\\n\" % level\n        )\n\n    sys.stderr.close()\n    if only_total:\n        if show_swap:\n            accuracy = swap_inacc\n        else:\n            accuracy = ram_inacc\n        if accuracy != 2:\n            sys.exit(1)\n\n\ndef get_memory_usage(pids_to_show, split_args, discriminate_by_pid,\n                    include_self=False, only_self=False):\n    cmds = {}\n    shareds = {}\n    shared_huges = {}\n    mem_ids = {}\n    count = {}\n    swaps = {}\n    for pid in os.listdir(proc.path('')):\n        if not pid.isdigit():\n            continue\n        pid = int(pid)\n\n        # Some filters\n        if only_self and pid != our_pid:\n            continue\n        if pid == our_pid and not include_self:\n            continue\n        if pids_to_show and pid not in pids_to_show:\n            continue\n\n        try:\n            cmd = getCmdName(pid, split_args, discriminate_by_pid)\n        except LookupError:\n            #operation not permitted\n            #kernel threads don't have exe links or\n            #process gone\n            continue\n\n        try:\n            private, shared, shared_huge, swap, mem_id = getMemStats(pid)\n        except RuntimeError:\n            continue #process gone\n        if shareds.get(cmd):\n            if have_pss: #add shared portion of PSS together\n                shareds[cmd] += shared\n            elif shareds[cmd] &lt; shared: #just take largest shared val\n                shareds[cmd] = shared\n        else:\n            shareds[cmd] = shared\n        if shared_huges.get(cmd):\n            if shared_huges[cmd] &lt; shared_huge: #just take largest shared_huge\n                shared_huges[cmd] = shared_huge\n        else:\n            shared_huges[cmd] = shared_huge\n        cmds[cmd] = cmds.setdefault(cmd, 0) + private\n        if cmd in count:\n            count[cmd] += 1\n        else:\n            count[cmd] = 1\n        mem_ids.setdefault(cmd, {}).update({mem_id: None})\n\n        # Swap (overcounting for now...)\n        swaps[cmd] = swaps.setdefault(cmd, 0) + swap\n\n    # Total swaped mem for each program\n    total_swap = 0\n\n    # Add shared mem for each program\n    total = 0\n\n    for cmd in cmds:\n        cmd_count = count[cmd]\n        if len(mem_ids[cmd]) == 1 and cmd_count &gt; 1:\n            # Assume this program is using CLONE_VM without CLONE_THREAD\n            # so only account for one of the processes\n            cmds[cmd] /= cmd_count\n            if have_pss:\n                shareds[cmd] /= cmd_count\n        # overestimation possible if shared_huges shared across commands\n        shareds[cmd] += shared_huges[cmd]\n        cmds[cmd] = cmds[cmd] + shareds[cmd]\n        total += cmds[cmd]  # valid if PSS available\n        total_swap += swaps[cmd]\n\n    sorted_cmds = sorted(cmds.items(), key=lambda x:x[1])\n    sorted_cmds = [x for x in sorted_cmds if x[1]]\n\n    return sorted_cmds, shareds, count, total, swaps, total_swap\n\ndef print_header(show_swap, discriminate_by_pid):\n    output_string = \" Private  +   Shared  =  RAM used\"\n    if show_swap:\n        output_string += \"   Swap used\"\n    output_string += \"\\tProgram\"\n    if discriminate_by_pid:\n        output_string += \"[pid]\"\n    output_string += \"\\n\\n\"\n    sys.stdout.write(output_string)\n\n\ndef print_memory_usage(sorted_cmds, shareds, count, total, swaps, total_swap,\n                    show_swap):\n    for cmd in sorted_cmds:\n\n        output_string = \"%9s + %9s = %9s\"\n        output_data = (human(cmd[1]-shareds[cmd[0]]),\n                    human(shareds[cmd[0]]), human(cmd[1]))\n        if show_swap:\n            output_string += \"   %9s\"\n            output_data += (human(swaps[cmd[0]]),)\n        output_string += \"\\t%s\\n\"\n        output_data += (cmd_with_count(cmd[0], count[cmd[0]]),)\n\n        sys.stdout.write(output_string % output_data)\n\n    # Only show totals if appropriate\n    if have_swap_pss and show_swap:  # kernel will have_pss\n        sys.stdout.write(\"%s\\n%s%9s%s%9s\\n%s\\n\" %\n                        (\"-\" * 45, \" \" * 24, human(total), \" \" * 3,\n                        human(total_swap), \"=\" * 45))\n    elif have_pss:\n        sys.stdout.write(\"%s\\n%s%9s\\n%s\\n\" %\n                        (\"-\" * 33, \" \" * 24, human(total), \"=\" * 33))\n\n\ndef verify_environment(pids_to_show):\n    if os.geteuid() != 0 and not pids_to_show:\n        sys.stderr.write(\"Sorry, root permission required, or specify pids with -p\\n\")\n        sys.stderr.close()\n        sys.exit(1)\n\n    try:\n        kernel_ver()\n    except (IOError, OSError):\n        val = sys.exc_info()[1]\n        if val.errno == errno.ENOENT:\n            sys.stderr.write(\n            \"Couldn't access \" + proc.path('') + \"\\n\"\n            \"Only GNU/Linux and FreeBSD (with linprocfs) are supported\\n\")\n            sys.exit(2)\n        else:\n            raise\n\ndef main():\n    # Force the stdout and stderr streams to be unbuffered\n    sys.stdout = Unbuffered(sys.stdout)\n    sys.stderr = Unbuffered(sys.stderr)\n\n    split_args, pids_to_show, watch, only_total, discriminate_by_pid, \\\n    show_swap = parse_options()\n\n    verify_environment(pids_to_show)\n\n    if not only_total:\n        print_header(show_swap, discriminate_by_pid)\n\n    if watch is not None:\n        try:\n            sorted_cmds = True\n            while sorted_cmds:\n                sorted_cmds, shareds, count, total, swaps, total_swap = \\\n                    get_memory_usage(pids_to_show, split_args,\n                                    discriminate_by_pid)\n                if only_total and show_swap and have_swap_pss:\n                    sys.stdout.write(human(total_swap, units=1)+'\\n')\n                elif only_total and not show_swap and have_pss:\n                    sys.stdout.write(human(total, units=1)+'\\n')\n                elif not only_total:\n                    print_memory_usage(sorted_cmds, shareds, count, total,\n                                    swaps, total_swap, show_swap)\n\n                sys.stdout.flush()\n                time.sleep(watch)\n            else:\n                sys.stdout.write('Process does not exist anymore.\\n')\n        except KeyboardInterrupt:\n            pass\n    else:\n        # This is the default behavior\n        sorted_cmds, shareds, count, total, swaps, total_swap = \\\n            get_memory_usage(pids_to_show, split_args,\n                            discriminate_by_pid)\n        if only_total and show_swap and have_swap_pss:\n            sys.stdout.write(human(total_swap, units=1)+'\\n')\n        elif only_total and not show_swap and have_pss:\n            sys.stdout.write(human(total, units=1)+'\\n')\n        elif not only_total:\n            print_memory_usage(sorted_cmds, shareds, count, total, swaps,\n                            total_swap, show_swap)\n\n    # We must close explicitly, so that any EPIPE exception\n    # is handled by our excepthook, rather than the default\n    # one which is reenabled after this script finishes.\n    sys.stdout.close()\n\n    ram_accuracy, swap_accuracy = val_accuracy( show_swap )\n    show_val_accuracy( ram_accuracy, swap_accuracy, only_total, show_swap )\n\nif __name__ == '__main__': main()\n</code></pre>"},{"location":"linux/rbash/","title":"Rbash","text":"<p>Restrict the Normal User to run only limited set of Commands</p>"},{"location":"linux/rsync/","title":"Rsync","text":"<p>rsync -avzh /home/sites/recipecreek.com/wp-content/uploads/2019 mk@18.215.135.107:/home/mk/sites_backup/sites/recipecreek.com/wp-content/uploads/</p>"},{"location":"linux/screen/","title":"Linux Screen command","text":"<pre><code>screen -ls\nscrren -s name\nscreen -r -- to reattach\nscreen -d  -- to detach\n\ncontrol -d to terminate\n</code></pre>"},{"location":"linux/sftp/","title":"Sftp","text":""},{"location":"linux/sftp/#_2","title":"Sftp","text":"<p>sftp Refer Link</p>"},{"location":"linux/sftp/#commands-to-change-permission","title":"Commands to change permission","text":"<ul> <li>vi /etc/ssh/sshd_config</li> </ul>"},{"location":"linux/sftp/#set-permission-for-main-root-folder-to-access","title":"Set Permission for Main Root folder to access","text":"<ul> <li>ls -ld /home/root_dir   </li> <li>chown root:root /home/root_dir</li> <li>chmod 755 -R /home/root_dir</li> </ul>"},{"location":"linux/sftp/#set-subfolder-access-to-their-respective-folder","title":"Set subfolder access to their respective folder","text":"<ul> <li>mkdir /home/root_dir/user_folder</li> <li>chown username:username /home/root_dir/user_folder</li> <li>chmod 700 user_folder</li> </ul>"},{"location":"linux/sftp/#add-these-lines-inside-sshd_config-file","title":"Add these lines inside sshd_config file","text":"sshd_config <pre><code>Match User  user or group=\nChrootDirectory /home/ (or)  add %u or %h\nForceCommand internal-sftp\nX11Forwarding no\nAllowTcpForwarding no\n</code></pre> <p>Notes</p> points <pre><code>%u - username\n%h - host\n</code></pre> <p>Notes</p> sftp-sshd_configREADME <pre><code>echo \"Enter Group Name To Create for sftp user\"\n\n# read sftpgroup\n# sftpgroup=\"\"\nsftpgroup=\"sftpgroup_resticted\"\ngroupadd $sftpgroup\n\necho \"Entered User names\" \necho $@\na=(\"$@\")\n\necho \"Permission Updated for respective User's\"\n\nfor names in \"${!a[@]}\"\ndo\n     Username=${a[$names]}\n    #  echo \"$nam\"\n     ls -ld /home/$Username\n    chown root:$TOKEN /home/$Username\n    chmod 775 /home/$Username\n    sudo usermod -a -G $sftpgroup $Username\ndone\n\necho \"Updating SSh config file for sftp users\"\n\ncat &lt;&lt;EOF &gt;&gt; /5\nMatch Group sftpuser\n    ChrootDirectory /home/%u\n    ForceCommand internal-sftp\n    X11Forwarding no\n    AllowTcpForwarding no\nEOF\n\nservice sshd restart\n\nb=\"$?\"\necho \"$b\"\nif [ $b == 0 ]\nthen   \n    echo \"Update Done\"\nelse\n    echo \"Update not done\"\nfi\n</code></pre> <pre><code>Readme\n\nloop through an array \nloop through array indices\n\n${array[@]}  array\n${!array[@]} array indices\n</code></pre>"},{"location":"linux/sftp/#folder-access-structure","title":"Folder access structure","text":"<p>Main root folder will be handle by sftp tp to restrict user login, and subfolder for the appropriate user will be created to give access for user's by mapping user name in sshd_config</p> <p>``` mermaid stateDiagram-v2   state fork_state &lt;&gt;     Root_Folder --&gt; fork_state     fork_state --&gt; sub1     fork_state --&gt; sub2 <pre><code>state join_state &lt;&lt;join&gt;&gt;\nsub1 --&gt; join_state\nsub2 --&gt; join_state\njoin_state --&gt; SFTP\nSFTP --&gt; [*]\n</code></pre> <p>```</p>"},{"location":"linux/shell_restrict/","title":"Shell_restrict","text":"<p>To restrict a Linux user to specific commands in bash_profile shell on CentOS 7, you can follow these steps:</p> <p>1.Open the bash_profile file for the user you want to restrict:</p> <p>Commands</p> <p>sudo nano /home/username/.bash_profile</p> <p>Replace \"username\" with the actual username.</p> <p>2.Add the following lines to the file:</p>"},{"location":"linux/shell_restrict/#restrict-user-to-specific-commands","title":"Restrict user to specific commands","text":"<pre><code>if [ \"$(id -u)\" != \"0\" ]; then\n  alias ls=\"ls --color=auto\"\n  alias ll=\"ls -l --color=auto\"\n  alias grep=\"grep --color=auto\"\n  alias ps=\"ps aux\"\n  alias top=\"top -o %CPU\"\n  alias df=\"df -h\"\n  alias du=\"du -h\"\n  alias free=\"free -h\"\n  alias ifconfig=\"ifconfig -a\"\n  alias netstat=\"netstat -antup\"\n  alias ping=\"ping -c 5\"\n  alias traceroute=\"traceroute -n\"\n  alias ssh=\"echo 'Access denied'\"\n  alias sudo=\"echo 'Access denied'\"\n  alias su=\"echo 'Access denied'\"\nfi\n</code></pre> <ul> <li>These lines create aliases for some common commands that the user is allowed to use, and block some other commands like ssh, sudo, and su.</li> </ul> <p>3.Save and close the file.</p> <p>4.Reload the bash_profile file:</p> <ul> <li> <p>source /home/username/.bash_profile</p> </li> <li> <p>Again, replace \"username\" with the actual username.</p> </li> </ul>"},{"location":"linux/squid/","title":"Squid installation &amp; config file","text":"<p>yum install squid httpd-tools sudo cp /etc/squid/squid.conf /etc/squid/squid.conf.default</p> <p>htpasswd -cb /etc/squid/passwd proxytunnel VtB35VJ5pnm31NwL9G cat /dev/null &gt; /etc/squid/conf.d/ipconfig.conf service squid restart</p>"},{"location":"linux/squid/#put-below-in-config-location","title":"Put below in config location","text":""},{"location":"linux/squid/#config","title":"Config","text":"<pre><code>acl manager proto cache_object\n#acl localhost src 136.232.211.158/32  14.98.54.90/32 202.83.25.120/32\n\nacl localhost src 127.0.0.0/8 \nacl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1\nacl localnet src 0.0.0.0/8\ninclude /etc/squid/conf.d/*.conf \n\n##########\n\n##########\n\nacl Safe_ports port 4377         #squid\nacl SSL_ports port 443\nacl Safe_ports port 80          # http\nacl Safe_ports port 21          # ftp\nacl Safe_ports port 443         # https\nacl Safe_ports port 70          # gopher\nacl Safe_ports port 210         # wais\nacl Safe_ports port 1025-65535  # unregistered ports\nacl Safe_ports port 280         # http-mgmt\nacl Safe_ports port 488         # gss-http\nacl Safe_ports port 591         # filemaker\nacl Safe_ports port 777         # multiling http\nacl CONNECT method CONNECT\n\nauth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd\nauth_param basic children 5\nauth_param basic realm Squid proxy-caching web server\nauth_param basic credentialsttl 2 hours\nauth_param basic casesensitive off\n\n\nacl ncsa_users proxy_auth REQUIRED\nhttp_access allow ncsa_users\n\n\nhttp_access allow manager localhost\nhttp_access deny manager\n\nhttp_access deny !Safe_ports\n\nhttp_access deny CONNECT !SSL_ports\n\n\n\nhttp_access allow localnet\nhttp_access allow localhost\n\nhttp_access deny all\n\nhttp_port 4377\n\nhierarchy_stoplist cgi-bin ?\n\n#cache_dir ufs /var/spool/squid 500 26 556\n\nforwarded_for delete\n\ncoredump_dir /var/spool/squid\n\nrefresh_pattern ^ftp:           1440    20%     10080\nrefresh_pattern ^gopher:        1440    0%      1440\nrefresh_pattern -i (/cgi-bin/|\\?) 0     0%      0\nrefresh_pattern .               0       20%     4320\n</code></pre>"},{"location":"linux/ssh-keygen/","title":"Ssh keygen","text":"<p>Notes</p> <ul> <li> <p>To generate an SSH key pair for login into a CentOS 7 server, you can follow these steps:</p> </li> <li> <p>Open a terminal on your local machine.</p> </li> <li> <p>Type the following command to generate a new SSH key pair:   <pre><code> ssh-keygen\n</code></pre></p> </li> <li> <p>This will prompt you for a file name to save the key pair, and a passphrase (which is optional). Press \"Enter\" to accept the default file name and location, and then enter a passphrase if you want to use one.</p> </li> <li> <p>Once the key pair has been generated, you should see two new files in the location you specified:</p> </li> </ul> <p>id_rsa (the private key)</p> <p>id_rsa.pub (the public key)</p> <ul> <li>Copy the public key to your CentOS 7 server by running the following command:\"</li> </ul> <pre><code>ssh-copy-id user@server-ip\n</code></pre> <ul> <li> <p>Replace user with your username on the CentOS 7 server, and server-ip with the IP address of the server.</p> </li> <li> <p>This command will copy the public key to the server and add it to the authorized_keys file, which will allow you to log in using the private key.</p> </li> <li> <p>Log in to the CentOS 7 server using the private key: <pre><code>ssh user@server-ip -i ~/.ssh/id_rsa\n</code></pre></p> </li> <li> <p>This command will log you in to the server using the private key that you just generated.</p> </li> <li> <p>That's it! You should now be able to log in to your CentOS 7 server using the SSH key pair.</p> </li> </ul> <p>================================</p> <ul> <li> <p>Connect to the server as the root user via SSH.</p> </li> <li> <p>Run the following command to generate the SSH key: <pre><code>ssh-keygen -t rsa -b 4096 -f /root/.ssh/id_rsa\n</code></pre></p> </li> <li> <p>This command will generate a 4096-bit RSA key and save it in the root user's .ssh directory with the filename id_rsa.</p> </li> <li> <p>When prompted, enter a passphrase for the key or leave it blank if you do not want to set one.</p> </li> <li> <p>Repeat step 2 to generate a second SSH key with a different name (e.g. id_rsa_second).</p> </li> <li> <p>Copy the public key of each SSH key pair (id_rsa.pub and id_rsa_second.pub) to the remote servers or services that you want to connect to using SSH.</p> </li> <li> <p>For example, to copy the id_rsa.pub file to a remote server, run the following command: <pre><code>ssh-copy-id -i /root/.ssh/id_rsa.pub user@remote-server\n</code></pre></p> </li> <li> <p>Replace user with the username on the remote server and remote-server with the hostname or IP address of the remote server.</p> </li> <li> <p>Repeat the command for each public key you want to copy to remote servers.</p> </li> <li> <p>Test the SSH connection using each SSH key by running the following command: <pre><code>ssh -i /root/.ssh/id_rsa user@remote-server\n</code></pre></p> </li> <li> <p>Replace id_rsa with the name of the SSH key file you want to use and user and remote-server with the appropriate values for the remote server.</p> </li> <li> <p>Repeat the command for each SSH key you want to test.</p> </li> <li> <p>That's it! You now have two SSH keys for the root user that you can use to securely connect to remote servers and services.</p> </li> </ul>"},{"location":"linux/sshd/","title":"Sshd","text":""},{"location":"linux/sshd/#_1","title":"Sshd","text":"<p>ssh superroot@67.202.121.30 -p9566 -J thilipsekar@202.94.174.48:4377</p> <p>bastion-proxy-ssh</p>"},{"location":"percona/commands/","title":"Commands","text":""},{"location":"percona/commands/#_1","title":"Commands","text":"<pre><code> curl -fsSL https://www.percona.com/get/pmm | /bin/bash\n</code></pre>"},{"location":"percona/commands/#to-change-the-user-name","title":"To Change the User Name","text":"<pre><code>docker exec pmm-server change-admin-password ihzJ4GrfPbmFNBY\n</code></pre>"},{"location":"questions/Docker/","title":"Docker","text":"<p>Write a Docker file</p> <p>Python as base image:  Install modules from requirements.txt Entry point start.py</p>"},{"location":"questions/Linux-qst/","title":"Linux qst","text":"<p>\ud835\uddd5\ud835\uddee\ud835\ude00\ud835\uddf6\ud835\uddf0\ud835\ude00: 1. <code>man</code> - Skim through manual for better understanding. 2. <code>nano</code> / <code>vim</code> - Use nano for basic editing, vim for advanced editing. 3. <code>ls -l</code> - List files with detailed info. 4. <code>ssh</code> - Connect to remote machines securely. 5. <code>df</code> / <code>du -hs *</code> - Check disk space usage. 6. <code>chown</code> / <code>chmod</code> - Change ownership and permissions of files. 7. <code>dig</code> - DNS lookup to find IPs of hostnames.</p> <p>\ud835\uddd8\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\ude06\ud835\uddf1\ud835\uddee\ud835\ude06 \ud835\udde8\ud835\ude00\ud835\uddf2: 1. <code>history</code> - View command history. 2. <code>cd -</code> - Switch to the previous directory. 3. <code>xargs</code> - Build and execute commands from input, with parallel execution support. 4. <code>pstree -p</code> - Display the process tree with process IDs. 5. <code>pgrep</code> / <code>pkill</code> - Find and signal processes by name.</p> <p>\ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddd9\ud835\uddf6\ud835\uddf9\ud835\uddf2\ud835\ude00 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee: 1. <code>find . -iname '*file*'</code> - Locate files by name. 2. <code>grep</code> / <code>ack</code> / <code>ag</code> - Search within files, with ag being the fastest. 3. <code>awk</code> / <code>sed</code> - Manipulate and format text. 4. <code>sort</code> / <code>uniq</code> - Sort and find unique lines. 5. <code>cut</code> / <code>paste</code> - Extract or combine columns of text.</p> <p>\ud835\udde6\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa \ud835\uddd7\ud835\uddf2\ud835\uddef\ud835\ude02\ud835\uddf4\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf4: 1. <code>top</code> / <code>htop</code> - View system performance and resource usage. 2. <code>netstat -lntp</code> - List all listening ports and associated processes. 3. <code>dstat</code> - Combined system stats view. 4. <code>strace</code> - Trace system calls and signals. 5. <code>lsof</code> - List open files and associated processes.</p>"},{"location":"questions/Objects/","title":"Objects","text":""},{"location":"questions/Objects/#1-workload-objects","title":"1. Workload Objects","text":"<p>Workload objects define the applications and jobs running in the cluster.</p> <ul> <li>Pod: The smallest deployable unit in Kubernetes. A pod runs one or more containers.</li> <li>ReplicaSet: Ensures a specified number of pod replicas are running at any given time.</li> <li>Deployment: Manages ReplicaSets and provides declarative updates for pods and ReplicaSets.</li> <li>StatefulSet: Manages stateful applications with persistent storage and unique pod identities.</li> <li>DaemonSet: Ensures that a copy of a pod runs on all or specific nodes in the cluster.</li> <li>Job: Creates pods to perform a specific task and ensures they complete successfully.</li> <li>CronJob: Manages jobs that run on a schedule, similar to a cron task.</li> </ul>"},{"location":"questions/Objects/#2-service-and-networking-objects","title":"2. Service and Networking Objects","text":"<p>These objects define how applications communicate internally and externally.</p> <ul> <li>Service**: Exposes an application running on a set of pods as a network service.</li> <li>Ingress**: Manages external HTTP and HTTPS access to services within the cluster.</li> <li>EndpointSlice: Tracks IP addresses and ports of services, supporting scalability.</li> <li>NetworkPolicy: Defines rules for controlling network traffic to/from pods.</li> </ul>"},{"location":"questions/Objects/#3-configuration-objects","title":"3. Configuration Objects","text":"<p>Configuration objects store and manage configuration data for applications.</p> <ul> <li>ConfigMap: Stores non-sensitive configuration data as key-value pairs.</li> <li>Secret: Stores sensitive information like passwords, tokens, and keys in an encrypted format.</li> <li>Volume: Provides storage to pods, supporting different storage backends.</li> <li>PersistentVolume (PV): Represents a storage resource in the cluster.</li> <li>PersistentVolumeClaim (PVC): Requests storage resources from PVs.</li> </ul>"},{"location":"questions/Objects/#4-cluster-management-objects","title":"4. Cluster Management Objects","text":"<p>These objects control the behavior and configuration of the Kubernetes cluster.</p> <ul> <li>Namespace: Provides logical isolation of resources within a cluster.</li> <li>Node: Represents a worker machine in the cluster.</li> <li>HorizontalPodAutoscaler (HPA): Automatically scales the number of pods in a deployment or replica set based on resource usage.</li> <li>VerticalPodAutoscaler (VPA): Adjusts resource requests and limits for pods.</li> <li>Role and RoleBinding: Define permissions within a namespace.</li> <li>ClusterRole and ClusterRoleBinding: Define permissions across the entire cluster.</li> </ul>"},{"location":"questions/Objects/#5-observability-and-debugging-objects","title":"5. Observability and Debugging Objects","text":"<p>These objects facilitate monitoring, logging, and troubleshooting.</p> <ul> <li>Event: Records changes or errors in the cluster.</li> <li>PodDisruptionBudget (PDB): Limits the number of pods that can be down during maintenance.</li> <li>Probe (Readiness, Liveness, Startup): Defines health checks for containers.</li> </ul> <p>Custom Resource Definitions (CRDs) - CRDs: Extend Kubernetes by defining your own objects. For example, in Argo Workflows, objects like Workflow and WorkflowTemplate are CRDs.</p>"},{"location":"questions/Python/","title":"Python","text":""},{"location":"questions/Python/#scenario-question","title":"scenario question","text":"<p>1 Scenario Looking to cleanup file in s3 with the specific tag \u2014 encrypted</p> <p>{[{\"arn\": \"s3:uswesr10290\", \"name\": \"s3_1\", \"tags\": [\"encrypted\"]}, {\"arn\": \"s3:uswesr10090\", \"name\": \"s3_2\", \"tags\": [\"db\"]}]} List one contains which have the tag encrypted List tow contains which do not have tag db</p> <p>Answer: </p> <p>import boto3</p> <p>def list_s3_buckets_tags():   s3_client = boto3.client('s3')   encrypted_buckets = []   non_encrypted_buckets = []s</p> <p>try:     response = s3_client.list_buckets()     buckets = response[Buckets']</p> <p>for bucket in buckets:       bucket_name = bucket['Name']       try:         tags = s3client.get_bucket_tagging(Bucket=bucket_name)         tag_set = tags['Set']         if any(tag['Key'] == 'encrypted' for tag in tag_set):           encrypted_buckets.append({             'arn'{bucket_name}\",             'name': bucket_name,             'tags': ['encrypted']           })        else:           non_encryptedbuckets.append({             'arn'bucket_name}\",             'name': bucket_name,             tags': [tagKey'] for tag in tag_set]           })</p> <p>return encrypted_buckets, non_encrypted_buckets</p> <p>if name == \"main\":   encrypted, non_encrypted = list_s3_buckets_tags()</p> <p>print(\"Buckets with 'encrypted' tag\")   for bucket in encrypted:    print(bucket)</p> <p>print(\"Buckets without 'encrypted' tag\")   for bucket in non_encrypted:    print(bucket)</p>"},{"location":"questions/aws/","title":"AWS","text":"<p>**Some AWS Interview Question****</p> <ol> <li>What is the role of IAM roles and policies?</li> <li>Can you explain the Terraform plan and its purpose?</li> <li>What is AWS Lambda, and how does it work?</li> <li>How do you invoke a Lambda function, and where do you configure it?</li> <li>Can you describe how Lambda handles scaling and event-based invocations?</li> <li>What is Amazon CloudWatch, and have you configured any custom metrics?</li> <li>What metrics are available on your CloudWatch dashboard?</li> <li>How do you configure CPU utilization on your CloudWatch dashboard?</li> <li>How do you attach an SSL certificate to an S3 bucket?</li> <li>What type of encryption have you implemented in your project?</li> <li>If an S3 bucket has a read-only policy, can you modify objects in the bucket?</li> <li>Why did you choose Terraform over Boto3 for infrastructure provisioning?</li> <li>What is a Content Delivery Network (CDN), and how does it work?</li> <li>Have you created a Jenkins pipeline for your project?</li> <li>How do you attach policies to IAM users, either individually or by group?</li> <li>What type of deployment strategies are you using in your project?</li> <li>Have you used any tools to create customized Amazon Machine Images (AMIs)?</li> <li>What is connection draining, and how does it work?</li> <li>How does an Elastic Load Balancer (ELB) distribute traffic?</li> <li>What is auto-scaling, and how does it work?</li> <li>Can you describe the different types of Load Balancers and provide examples?</li> <li>What is the maximum runtime for a Lambda function?</li> <li>What is the maximum memory size for a Lambda function?</li> <li>How can you increase the runtime for a Lambda function?</li> <li>What automations have you performed using Lambda in your project?</li> <li>Why did you choose Terraform over Boto3 for infrastructure provisioning?</li> <li>What modules have you used in your Lambda function?</li> <li>Have you created an SNS topic for your project?</li> <li>If you've exhausted IP addresses in your VPC, how would you provision new resources?</li> <li>What is Groovy, and how is it used in Jenkins?</li> <li>Why do you use Groovy in Jenkins, and where do you save Jenkins files?</li> <li>What is Ansible, and what is its purpose?</li> <li>What language do you use in Ansible?</li> <li>Where do you run Terraform code, remotely or locally?</li> <li>What is the purpose of access keys and secret keys in AWS?</li> <li>What are Terraform modules, and have you used any in your project?</li> <li>What environments have you set up for your project?</li> <li>Do you use the same AWS account for all environments?</li> <li>Do you have separate Jenkins servers for each environment?</li> <li>Where do you write and save your Lambda function code?</li> </ol>"},{"location":"questions/int/","title":"Int","text":""},{"location":"questions/int/#l1-int-question","title":"L1 Int question","text":""},{"location":"questions/int/#linux","title":"linux","text":"<p>command to show the space how to add user &amp; want custom path for the user</p> <p>what are the objects used to create pod, can tell the objects kubernetes rbac</p> <p>helm rendered version, which is getting applied. [Answer Helm Template]</p> <p>print only the file names [ ls -lrt | awk '{print $9}' ]</p> <p>i have multiple file names under a folder write a bash to identify if the folder has to file if yes print it as found </p> <p><code>bash</code></p>"},{"location":"questions/int/#binbash","title":"!/bin/bash","text":""},{"location":"questions/int/#set-the-directory-path","title":"Set the directory path","text":"<p>DIR_PATH=\"/path/to/your/folder\"</p>"},{"location":"questions/int/#set-the-filenames-you-want-to-check-for","title":"Set the filenames you want to check for","text":"<p>FILE1=\"file1.txt\" FILE2=\"file2.txt\"</p>"},{"location":"questions/int/#check-if-both-files-exist","title":"Check if both files exist","text":"<p>if [[ -f \"$DIR_PATH/$FILE1\" &amp;&amp; -f \"$DIR_PATH/$FILE2\" ]]; then     echo \"found\" else     echo \"One or both files not found\" fi</p>"},{"location":"questions/int/#terraform","title":"Terraform","text":"<p>why do we use data block in terraform </p> <p>[ Data sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions. ]</p> <p>Local Values</p> <p>A local value assigns a name to an expression, so you can use the name multiple times within a module instead of repeating the expression.</p> <p>in terraform plan it shows 10 changes but we have to apply only particular changes [</p> <pre><code>In Terraform, if you only want to apply specific changes and not all the changes shown in a plan, you can achieve this by:\n</code></pre> <p>Using Targeted Resource Apply (-target): You can use the -target option to apply changes only to specific resources. For example:</p> <p>bash Copy code terraform apply -target=module.module_name.resource_type.resource_name This command will only apply the changes for the targeted resource(s) while ignoring other changes in the plan.</p> <p>Manually Apply Specific Changes: If the change involves editing the resource configuration, you can manually adjust the configuration to include only the changes you want and then run terraform apply as usual.</p> <p>Selective State Management: If needed, you can use terraform state commands to manage resources selectively. For instance, you can use terraform state rm to remove resources from the state file temporarily, apply changes, and then re-import them using terraform import.</p> <p>Running Plan and Inspecting Changes: Before applying any changes, run:</p> <p>bash Copy code terraform plan -target=module.module_name.resource_type.resource_name This will generate a plan specific to your targeted resources.</p> <p>Each approach allows you to narrow down the scope of your changes while applying only what\u2019s needed.</p> <p>]</p>"},{"location":"questions/int/#_1","title":"Int","text":"<p>Cluster add-ons [] liveness and rediness</p>"},{"location":"questions/int/#jenkins","title":"jenkins","text":"<p>DSL Concept how to test different environments</p> <p>jenkins executor</p>"},{"location":"questions/kubernetes-int/","title":"Kubernetes int","text":""},{"location":"questions/kubernetes-int/#kubernetes-questions","title":"Kubernetes-questions","text":"<ol> <li> <p>Create a yaml file for pod in cluster</p> </li> <li> <p>Kubernetes objects <pre><code>Workload objects define the applications and jobs running in the cluster.\n\nPod: The smallest deployable unit in Kubernetes. A pod runs one or more containers.\nReplicaSet: Ensures a specified number of pod replicas are running at any given time.\nDeployment: Manages ReplicaSets and provides declarative updates for pods and ReplicaSets.\nStatefulSet: Manages stateful applications with persistent storage and unique pod identities.\nDaemonSet: Ensures that a copy of a pod runs on all or specific nodes in the cluster.\nJob: Creates pods to perform a specific task and ensures they complete successfully.\nCronJob: Manages jobs that run on a schedule, similar to a cron task.\n</code></pre></p> </li> <li> <p>Few Kubernetes commands:</p> </li> <li> <p>Sceanrio based question Namespace call tata, there are many service under namespace. Some services failed no pro active alerts  Get list of pods that are not in running state, and in-incomplete pods</p> </li> </ol> <pre><code>kubectl get pods -n tata --field-selector=status.phase!=Running\n</code></pre> <ol> <li> <p>Faulty node one our cluster service hosting in the cluster not responding auto to not fault, I have ip of the node, we wan\u2019t to find the services running in node <pre><code>kubectl get pods -o wide --all-namespaces | grep &lt;node-ip&gt;\n\n##Find Services Linked to Those Pods##\n\nkubectl get svc --all-namespaces | grep &lt;pod-name&gt;\n</code></pre></p> </li> <li> <p>List pods with nodes running <pre><code>kubectl get pods -o wide\n    Alternaive\nkubectl get pods -o custom-columns=\"NAME:.metadata.name,NODE:.spec.nodeName\"\n</code></pre></p> </li> </ol> <p>1) Command to list pods running in which node and which availability zone:  <pre><code> kubectl get pods -o custom-columns=\"POD:metadata.name,NODE:spec.nodeName\" | tail -n +2 | while read pod node; do\n echo -n \"$pod $node \"\n kubectl get node \"$node\" -o jsonpath=\"{.metadata.labels.topology\\.kubernetes\\.io/zone}\"\n echo \"\"\n done\n</code></pre></p> <p>2) List of nodes and how many pods running on them: <pre><code>kubectl get po -o json --all-namespaces | \\\n jq '.items | group_by(.spec.nodeName) | map({\"nodeName\": .[0].spec.nodeName, \"count\": length}) | sort_by(.count)'\n</code></pre> 3) List pods using most of RAM and CPU: For CPU: <pre><code>kubectl top pods -A | sort --reverse --key 3 --numeric\n</code></pre> For RAM: <pre><code>kubectl top pods -A | sort --reverse --key 4 --numeric\n</code></pre></p> <p>4) Getting pods that are continuously restarting (sorting them): <pre><code>kubectl get pods --all-namespaces -o json | jq -r '.items | sort_by(.status.containerStatuses[0].restartCount) | reverse[] | [.metadata.namespace, .metadata.name, .status.containerStatuses[0].restartCount] | @tsv' | column -t\n</code></pre> 5) Quickly check the pod limits: <pre><code>kubectl get pods -o=custom-columns='NAME:spec.containers[*].name,MEMREQ:spec.containers[*].resources.requests.memory,MEMLIM:spec.containers[*].resources.limits.memory,CPUREQ:spec.containers[*].resources.requests.cpu,CPULIM:spec.containers[*].resources.limits.cpu'\n</code></pre> 6) Get all private IPs of nodes: <pre><code>kubectl get nodes -o json | \\\n jq -r '.items[].status.addresses[]? | select (.type == \"InternalIP\") | .address' | \\\n paste -sd \"\\n\" -\n</code></pre> 7) Checking logs: Read logs with human readable timestamp: <pre><code>kubectl logs -f my-pod --timestamps\n</code></pre> 8. Logs of the pods, last 100 lines \u2014tail</p> <p><pre><code>kubectl logs -f my-pod --tail=100\n</code></pre> 9) Check for events across all namespaces and filter for any errors, <pre><code>kubectl get events --all-namespaces --field-selector type=Warning -o wide\nor \nkubectl get events --all-namespaces --field-selector type!=Normal -o wide\n</code></pre></p>"},{"location":"questions/kubernetes-int/#kubernetes-commands","title":"Kubernetes Commands","text":""},{"location":"questions/kubernetes-int/#1-cluster-information","title":"1. Cluster Information","text":"<ul> <li>View Cluster Information:   <pre><code>kubectl cluster-info\n</code></pre></li> <li>List All Nodes:   <pre><code>kubectl get nodes\n</code></pre></li> <li>Describe a Node:   <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#2-pod-management","title":"2. Pod Management","text":"<ul> <li>List All Pods:   <pre><code>kubectl get pods\n</code></pre></li> <li>List Pods in a Specific Namespace:   <pre><code>kubectl get pods -n &lt;namespace&gt;\n</code></pre></li> <li>View Pod Logs:   <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre></li> <li>View Logs for a Specific Container in a Pod:   <pre><code>kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;\n</code></pre></li> <li>Describe a Pod:   <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre></li> <li>Execute a Command in a Pod:   <pre><code>kubectl exec -it &lt;pod-name&gt; -- &lt;command&gt;\n</code></pre></li> <li>Delete a Pod:   <pre><code>kubectl delete pod &lt;pod-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#3-deployment-management","title":"3. Deployment Management","text":"<ul> <li>List Deployments:   <pre><code>kubectl get deployments\n</code></pre></li> <li>Describe a Deployment:   <pre><code>kubectl describe deployment &lt;deployment-name&gt;\n</code></pre></li> <li>Scale a Deployment:   <pre><code>kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;number&gt;\n</code></pre></li> <li>Apply Changes from a YAML File:   <pre><code>kubectl apply -f &lt;file.yaml&gt;\n</code></pre></li> <li>Roll Back a Deployment:   <pre><code>kubectl rollout undo deployment &lt;deployment-name&gt;\n</code></pre></li> <li>View Deployment History:   <pre><code>kubectl rollout history deployment &lt;deployment-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#4-service-and-networking","title":"4. Service and Networking","text":"<ul> <li>List Services:   <pre><code>kubectl get svc\n</code></pre></li> <li>Describe a Service:   <pre><code>kubectl describe svc &lt;service-name&gt;\n</code></pre></li> <li>Port Forward a Service:   <pre><code>kubectl port-forward svc/&lt;service-name&gt; &lt;local-port&gt;:&lt;service-port&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#5-namespace-management","title":"5. Namespace Management","text":"<ul> <li>List Namespaces:   <pre><code>kubectl get namespaces\n</code></pre></li> <li>Create a Namespace:   <pre><code>kubectl create namespace &lt;namespace-name&gt;\n</code></pre></li> <li>Delete a Namespace:   <pre><code>kubectl delete namespace &lt;namespace-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>List All Events:   <pre><code>kubectl get events\n</code></pre></li> <li>Check the Status of All Resources:   <pre><code>kubectl get all\n</code></pre></li> <li>Debug a Node:   <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></li> <li>Debug a Pod:   <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#7-resource-management","title":"7. Resource Management","text":"<ul> <li>Get Resource Usage (using Metrics Server):   <pre><code>kubectl top nodes\nkubectl top pods\n</code></pre></li> <li>Delete a Resource:   <pre><code>kubectl delete &lt;resource-type&gt; &lt;resource-name&gt;\n</code></pre></li> <li>Edit a Resource:   <pre><code>kubectl edit &lt;resource-type&gt; &lt;resource-name&gt;\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#8-custom-resources-and-yaml-files","title":"8. Custom Resources and YAML Files","text":"<ul> <li>Validate a YAML File:   <pre><code>kubectl apply --dry-run=client -f &lt;file.yaml&gt;\n</code></pre></li> <li>Delete All Resources Defined in a File:   <pre><code>kubectl delete -f &lt;file.yaml&gt;\n</code></pre></li> <li>List Custom Resources (CRDs):   <pre><code>kubectl get crd\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#9-utility-commands","title":"9. Utility Commands","text":"<ul> <li>Switch Context:   <pre><code>kubectl config use-context &lt;context-name&gt;\n</code></pre></li> <li>View Current Context:   <pre><code>kubectl config current-context\n</code></pre></li> <li>List All Contexts:   <pre><code>kubectl config get-contexts\n</code></pre></li> </ul>"},{"location":"questions/kubernetes-int/#kubernetes-objects","title":"kubernetes Objects","text":"<p>(Kubernetes object)</p>"},{"location":"questions/kubernetes/","title":"Kubernetes","text":"<p>You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts.</p> <p>Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl.</p> <p>Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl.</p> <p>=====================</p> <p>Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on controlplane nodes. Do not add new labels to any nodes.</p> <p>=====================</p> <p>Use context: kubectl config use-context k8s-c1-H</p>"},{"location":"questions/kubernetes/#there-are-two-pods-named-o3db-in-namespace-project-c13-c13-management-asked-you-to-scale-the-pods-down-to-one-replica-to-save-resources","title":"There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>Do the following in Namespace default. Create a single Pod named ready-if-service-ready of image nginx:1.16.1-alpine. Configure a LivenessProbe which simply executes command true. Also configure a ReadinessProbe which does check if the url http://service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe.</p> <p>Create a second Pod named am-i-ready of image nginx:1.16.1-alpine with label id: cross-server-ready. The already existing Service service-am-i-ready should now have that second Pod as endpoint.</p>"},{"location":"questions/kubernetes/#now-the-first-pod-should-be-in-ready-state-confirm-that","title":"Now the first Pod should be in ready state, confirm that.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE (metadata.creationTimestamp).</p>"},{"location":"questions/kubernetes/#write-a-second-command-into-optcourse5find_pods_uidsh-which-lists-all-pods-sorted-by-field-metadatauid-use-kubectl-sorting-for-both-commands","title":"Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>Create a new PersistentVolume named safari-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined.</p> <p>Next create a new PersistentVolumeClaim in Namespace project-tiger named safari-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly.</p>"},{"location":"questions/kubernetes/#finally-create-a-new-deployment-safari-in-namespace-project-tiger-which-mounts-that-volume-at-tmpsafari-data-the-pods-of-that-deployment-should-be-of-image-httpd2441-alpine","title":"Finally create a new Deployment safari in Namespace project-tiger which mounts that volume at /tmp/safari-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>The metrics-server has been installed in the cluster. Your college would like to know the kubectl commands to:</p> <p>show Nodes resource usage show Pods and their containers resource usage Please write the commands into /opt/course/7/node.sh and /opt/course/7/pod.sh. ===================== Use context: kubectl config use-context k8s-c1-H</p> <p>Ssh into the controlplane node with ssh cluster1-controlplane1. Check how the controlplane components kubelet, kube-apiserver, kube-scheduler, kube-controller-manager and etcd are started/installed on the controlplane node. Also find out the name of the DNS application and how it's started/installed on the controlplane node.</p> <p>Write your findings into file /opt/course/8/controlplane-components.txt. The file should be structured like:</p> <pre><code># /opt/course/8/controlplane-components.txt\nkubelet: [TYPE]\nkube-apiserver: [TYPE]\nkube-scheduler: [TYPE]\nkube-controller-manager: [TYPE]\netcd: [TYPE]\ndns: [TYPE] [NAME]\n</code></pre>"},{"location":"questions/kubernetes/#choices-of-type-are-not-installed-process-static-pod-pod","title":"Choices of [TYPE] are: not-installed, process, static-pod, pod","text":"<p>Use context: kubectl config use-context k8s-c2-AC</p> <p>Ssh into the controlplane node with ssh cluster2-controlplane1. Temporarily stop the kube-scheduler, this means in a way that you can start it again afterwards.</p> <p>Create a single Pod named manual-schedule of image httpd:2.4-alpine, confirm it's created but not scheduled on any node.</p> <p>Now you're the scheduler and have all its power, manually schedule that Pod on node cluster2-controlplane1. Make sure it's running.</p>"},{"location":"questions/kubernetes/#start-the-kube-scheduler-again-and-confirm-its-running-correctly-by-creating-a-second-pod-named-manual-schedule2-of-image-httpd24-alpine-and-check-if-its-running-on-cluster2-node1","title":"Start the kube-scheduler again and confirm it's running correctly by creating a second Pod named manual-schedule2 of image httpd:2.4-alpine and check if it's running on cluster2-node1.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p>"},{"location":"questions/kubernetes/#create-a-new-serviceaccount-processor-in-namespace-project-hamster-create-a-role-and-rolebinding-both-named-processor-as-well-these-should-allow-the-new-sa-to-only-create-secrets-and-configmaps-in-that-namespace","title":"Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p>"},{"location":"questions/kubernetes/#use-namespace-project-tiger-for-the-following-create-a-daemonset-named-ds-important-with-image-httpd24-alpine-and-labels-idds-important-and-uuid18426a0b-5f59-4e10-923f-c0e078e82462-the-pods-it-creates-should-request-10-millicore-cpu-and-10-mebibyte-memory-the-pods-of-that-daemonset-should-run-on-all-nodes-also-controlplanes","title":"Use Namespace project-tiger for the following. Create a DaemonSet named ds-important with image httpd:2.4-alpine and labels id=ds-important and uuid=18426a0b-5f59-4e10-923f-c0e078e82462. The Pods it creates should request 10 millicore cpu and 10 mebibyte memory. The Pods of that DaemonSet should run on all nodes, also controlplanes.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>Use Namespace project-tiger for the following. Create a Deployment named deploy-important with label id=very-important (the Pods should also have this label) and 3 replicas. It should contain two containers, the first named container1 with image nginx:1.17.6-alpine and the second one named container2 with image google/pause.</p> <p>There should be only ever one Pod of that Deployment running on one worker node. We have two worker nodes: cluster1-node1 and cluster1-node2. Because the Deployment has three replicas the result should be that on both nodes one Pod is running. The third Pod won't be scheduled, unless a new worker node will be added. Use topologyKey: kubernetes.io/hostname for this.</p> <p>In a way we kind of simulate the behaviour of a DaemonSet here, but using a Deployment and a fixed number of replicas.</p> <p>===================== Use context: kubectl config use-context k8s-c1-H</p> <p>Create a Pod named multi-container-playground in Namespace default with three containers, named c1, c2 and c3. There should be a volume attached to that Pod and mounted into every container, but the volume shouldn't be persisted or shared with other Pods.</p> <p>Container c1 should be of image nginx:1.17.6-alpine and have the name of the node where its Pod is running available as environment variable MY_NODE_NAME.</p> <p>Container c2 should be of image busybox:1.31.1 and write the output of the date command every second in the shared volume into file date.log. You can use while true; do date &gt;&gt; /your/vol/path/date.log; sleep 1; done for this.</p> <p>Container c3 should be of image busybox:1.31.1 and constantly send the content of file date.log from the shared volume to stdout. You can use tail -f /your/vol/path/date.log for this.</p>"},{"location":"questions/kubernetes/#check-the-logs-of-container-c3-to-confirm-correct-setup","title":"Check the logs of container c3 to confirm correct setup.","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>You're ask to find out following information about the cluster k8s-c1-H :</p> <p>How many controlplane nodes are available? How many worker nodes are available? What is the Service CIDR? Which Networking (or CNI Plugin) is configured and where is its config file? Which suffix will static pods have that run on cluster1-node1? Write your answers into file /opt/course/14/cluster-info, structured like this:</p>"},{"location":"questions/kubernetes/#optcourse14cluster-info","title":"/opt/course/14/cluster-info","text":"<p>1: [ANSWER] 2: [ANSWER] 3: [ANSWER] 4: [ANSWER] 5: [ANSWER] ===================== Use context: kubectl config use-context k8s-c2-AC</p> <p>Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time (metadata.creationTimestamp). Use kubectl for it.</p> <p>Now delete the kube-proxy Pod running on node cluster2-node1 and write the events this caused into /opt/course/15/pod_kill.log.</p> <p>Finally kill the containerd container of the kube-proxy Pod on node cluster2-node1 and write the events into /opt/course/15/container_kill.log.</p>"},{"location":"questions/kubernetes/#do-you-notice-differences-in-the-events-both-actions-caused","title":"Do you notice differences in the events both actions caused?","text":"<p>Use context: kubectl config use-context k8s-c1-H</p> <p>Write the names of all namespaced Kubernetes resources (like Pod, Secret, ConfigMap...) into /opt/course/16/resources.txt.</p> <p>Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt.</p> <p>=====================</p> <p>Use context: kubectl config use-context k8s-c1-H</p> <p>In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod=container and container=pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod.</p> <p>Using command crictl:</p> <p>Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt</p> <p>Write the logs of the container into /opt/course/17/pod-container.log</p> <p>===================== Use context: kubectl config use-context k8s-c3-CCC</p> <p>There seems to be an issue with the kubelet not running on cluster3-node1. Fix it and confirm that cluster has node cluster3-node1 available in Ready state afterwards. You should be able to schedule a Pod on cluster3-node1 afterwards.</p>"},{"location":"questions/kubernetes/#write-the-reason-of-the-issue-into-optcourse18reasontxt","title":"Write the reason of the issue into /opt/course/18/reason.txt.","text":"<p>NOTE: This task can only be solved if questions 18 or 20 have been successfully implemented and the k8s-c3-CCC cluster has a functioning worker node</p> <p>Use context: kubectl config use-context k8s-c3-CCC</p> <p>Do the following in a new Namespace secret. Create a Pod named secret-pod of image busybox:1.31.1 which should keep running for some time.</p> <p>There is an existing Secret located at /opt/course/19/secret1.yaml, create it in the Namespace secret and mount it readonly into the Pod at /tmp/secret1.</p> <p>Create a new Secret in Namespace secret called secret2 which should contain user=user1 and pass=1234. These entries should be available inside the Pod's container as environment variables APP_USER and APP_PASS.</p>"},{"location":"questions/kubernetes/#confirm-everything-is-working","title":"Confirm everything is working.","text":"<p>Use context: kubectl config use-context k8s-c3-CCC</p>"},{"location":"questions/kubernetes/#your-coworker-said-node-cluster3-node2-is-running-an-older-kubernetes-version-and-is-not-even-part-of-the-cluster-update-kubernetes-on-that-node-to-the-exact-version-thats-running-on-cluster3-controlplane1-then-add-this-node-to-the-cluster-use-kubeadm-for-this","title":"Your coworker said node cluster3-node2 is running an older Kubernetes version and is not even part of the cluster. Update Kubernetes on that node to the exact version that's running on cluster3-controlplane1. Then add this node to the cluster. Use kubeadm for this.","text":"<p>Use context: kubectl config use-context k8s-c3-CCC</p> <p>Create a Static Pod named my-static-pod in Namespace default on cluster3-controlplane1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory.</p> <p>Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if it's reachable through the cluster3-controlplane1 internal IP address. You can connect to the internal node IPs from your main terminal.</p> <p>=====================</p> <p>Use context: kubectl config use-context k8s-c2-AC</p> <p>Check how long the kube-apiserver server certificate is valid on cluster2-controlplane1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration.</p> <p>Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date.</p>"},{"location":"questions/kubernetes/#write-the-correct-kubeadm-command-that-would-renew-the-apiserver-server-certificate-into-optcourse22kubeadm-renew-certssh","title":"Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh.","text":"<p>Use context: kubectl config use-context k8s-c2-AC</p> <p>Node cluster2-node1 has been added to the cluster using kubeadm and TLS bootstrapping.</p> <p>Find the \"Issuer\" and \"Extended Key Usage\" values of the cluster2-node1:</p> <p>kubelet client certificate, the one used for outgoing connections to the kube-apiserver. kubelet server certificate, the one used for incoming connections from the kube-apiserver. Write the information into file /opt/course/23/certificate-info.txt.</p> <p>Compare the \"Issuer\" and \"Extended Key Usage\" fields of both certificates and make sense of these.</p> <p>===================== Use context: kubectl config use-context k8s-c1-H</p> <p>There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod.</p> <p>To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to:</p> <p>connect to db1- Pods on port 1111 connect to db2- Pods on port 2222 Use the app label of Pods in your policy.</p> <p>After implementation, connections from backend- Pods to vault- Pods on port 3333 should for example no longer work.</p> <p>===================== Use context: kubectl config use-context k8s-c3-CCC</p> <p>Make a backup of etcd running on cluster3-controlplane1 and save it on the controlplane node at /tmp/etcd-backup.db.</p> <p>Then create any kind of Pod in the cluster.</p>"},{"location":"questions/kubernetes/#finally-restore-the-backup-confirm-the-cluster-is-still-working-and-that-the-created-pod-is-no-longer-with-us","title":"Finally restore the backup, confirm the cluster is still working and that the created Pod is no longer with us.","text":""},{"location":"questions/q1/","title":"DevOps Interview Questions and Answers","text":""},{"location":"questions/q1/#1-what-would-you-do-if-an-ec2-instance-is-getting-slow","title":"1. What would you do if an EC2 instance is getting slow?","text":"<ul> <li>Check CPU, Memory, and Disk Utilization using CloudWatch metrics.</li> <li>Investigate running processes to identify bottlenecks using tools like <code>htop</code> or <code>top</code>.</li> <li>Ensure the instance type is sufficient for the workload; consider upgrading if necessary.</li> <li>Look for network or I/O issues.</li> <li>Verify the application performance logs.</li> <li>Restart the instance as a last resort.</li> </ul>"},{"location":"questions/q1/#2-if-users-cant-access-an-application-hosted-on-ec2-what-steps-would-you-take","title":"2. If users can\u2019t access an application hosted on EC2, what steps would you take?","text":"<ul> <li>Verify the instance health in the AWS Console.</li> <li>Check security group and network ACL rules for proper port and IP configurations.</li> <li>Ensure the application service is running.</li> <li>Confirm the DNS configuration and connectivity.</li> <li>Investigate application logs for errors.</li> <li>Test the connection using tools like <code>curl</code> or <code>ping</code>.</li> </ul>"},{"location":"questions/q1/#3-whats-the-difference-between-a-load-balancer-and-a-reverse-proxy","title":"3. What\u2019s the difference between a Load Balancer and a Reverse Proxy?","text":"<ul> <li>Load Balancer distributes traffic across multiple servers for scalability and fault tolerance.</li> <li>Reverse Proxy acts as an intermediary server to forward client requests to backend servers, often providing caching, SSL termination, and additional security.</li> </ul>"},{"location":"questions/q1/#4-how-would-you-write-a-terraform-script-to-create-an-ec2-instance-and-run-a-script-on-every-reboot","title":"4. How would you write a Terraform script to create an EC2 instance and run a script on every reboot?","text":""},{"location":"questions/q1/#resource-aws_instance-example-ami-ami-12345678-instance_type-t2micro-user_data-eot-binbash-echo-script-runs-on-every-reboot-varlogrebootlog-eot-tags-name-example-instance","title":"<pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t2.micro\"\n\n  user_data = &lt;&lt;-EOT\n              #!/bin/bash\n              echo \"Script runs on every reboot\" &gt;&gt; /var/log/reboot.log\n              EOT\n\n  tags = {\n    Name = \"example-instance\"\n  }\n}\n</code></pre>","text":""},{"location":"questions/q1/#5-what-is-a-backend-in-terraform-and-why-is-it-used","title":"5. What is a Backend in Terraform, and why is it used?","text":"<ul> <li>Backend defines where Terraform stores its state data (e.g., local, S3, or Consul).</li> <li>It is used to enable state sharing, locking, and remote operations.</li> </ul>"},{"location":"questions/q1/#6-what-is-the-docker-lifecycle","title":"6. What is the Docker lifecycle?","text":"<ol> <li>Create</li> <li>Start</li> <li>Stop</li> <li>Restart</li> <li>Pause</li> <li>Unpause</li> <li>Remove</li> </ol>"},{"location":"questions/q1/#7-what-are-the-key-docker-components","title":"7. What are the key Docker components?","text":"<ul> <li>Docker Engine: The runtime environment.</li> <li>Docker Images: Templates for containers.</li> <li>Docker Containers: Running instances of images.</li> <li>Docker Compose: Tool for defining multi-container applications.</li> <li>Docker Hub: Registry for sharing images.</li> </ul>"},{"location":"questions/q1/#8-whats-the-difference-between-a-docker-image-and-a-docker-container","title":"8. What\u2019s the difference between a Docker Image and a Docker Container?","text":"<ul> <li>Docker Image: A read-only template with application code and dependencies.</li> <li>Docker Container: A running instance of a Docker image.</li> </ul>"},{"location":"questions/q1/#9-what-should-you-do-before-creating-a-docker-container","title":"9. What should you do before creating a Docker container?","text":"<ul> <li>Ensure the Docker image is properly built.</li> <li>Verify dependencies are included.</li> <li>Confirm the application is tested locally.</li> </ul>"},{"location":"questions/q1/#10-what-is-docker-compose-and-how-do-you-use-it","title":"10. What is Docker Compose, and how do you use it?","text":"<ul> <li>Docker Compose: A tool to define and run multi-container Docker applications.</li> <li>Use a <code>docker-compose.yml</code> file to define services and run <code>docker-compose up</code>.</li> </ul>"},{"location":"questions/q1/#11-what-steps-would-you-take-if-you-see-an-unhealthy-status-in-an-elb","title":"11. What steps would you take if you see an \"unhealthy\" status in an ELB?","text":"<ul> <li>Check target group health checks.</li> <li>Ensure instances are running and reachable.</li> <li>Verify security group rules.</li> <li>Look at application logs for errors.</li> </ul>"},{"location":"questions/q1/#12-how-do-you-optimize-docker-images-for-better-performance","title":"12. How do you optimize Docker images for better performance?","text":"<ul> <li>Use multi-stage builds.</li> <li>Minimize the number of layers.</li> <li>Avoid including unnecessary files.</li> <li>Use a minimal base image like <code>alpine</code>.</li> </ul>"},{"location":"questions/q1/#13-how-would-you-secure-a-docker-container","title":"13. How would you secure a Docker container?","text":"<ul> <li>Use non-root users.</li> <li>Limit container capabilities.</li> <li>Implement network policies.</li> <li>Scan images for vulnerabilities.</li> <li>Use signed images.</li> </ul>"},{"location":"questions/q1/#14-what-is-jenkins-scaling-and-how-do-you-achieve-it","title":"14. What is Jenkins scaling, and how do you achieve it?","text":"<ul> <li>Scaling Jenkins involves adding worker nodes to handle more builds.</li> <li>Use Jenkins distributed builds by configuring a master-slave architecture.</li> </ul>"},{"location":"questions/q1/#15-what-is-the-role-of-the-master-and-node-in-jenkins","title":"15. What is the role of the Master and Node in Jenkins?","text":"<ul> <li>Master: Orchestrates builds and provides the UI.</li> <li>Node: Executes build tasks.</li> </ul>"},{"location":"questions/q1/#16-what-is-a-sidecar-container-and-when-would-you-use-it","title":"16. What is a Sidecar container, and when would you use it?","text":"<ul> <li>A Sidecar container runs alongside a main application container, providing auxiliary services like logging, monitoring, or proxying.</li> </ul>"},{"location":"questions/q1/#17-what-is-the-difference-between-configmap-and-secrets-in-kubernetes","title":"17. What is the difference between ConfigMap and Secrets in Kubernetes?","text":"<ul> <li>ConfigMap: Stores non-sensitive configuration data.</li> <li>Secrets: Stores sensitive data like passwords or keys.</li> </ul>"},{"location":"questions/q1/#18-what-is-the-default-deployment-in-kubernetes","title":"18. What is the default deployment in Kubernetes?","text":"<ul> <li>Deployment: Manages stateless applications and ensures updates with rollbacks.</li> </ul>"},{"location":"questions/q1/#19-what-are-taints-and-tolerations-in-kubernetes","title":"19. What are Taints and Tolerations in Kubernetes?","text":"<ul> <li>Taints: Restrict which nodes can schedule pods.</li> <li>Tolerations: Allow pods to override taints.</li> </ul>"},{"location":"questions/q1/#20-what-is-a-static-pod-in-kubernetes-and-how-is-it-different-from-a-regular-pod","title":"20. What is a Static Pod in Kubernetes, and how is it different from a regular pod?","text":"<ul> <li>Static Pod: Managed directly by the kubelet, not by the API server.</li> <li>Regular pods are managed by the API server.</li> </ul>"},{"location":"questions/q1/#21-how-do-you-check-pod-logs-and-attach-prometheus-for-monitoring","title":"21. How do you check pod logs and attach Prometheus for monitoring?","text":"<ul> <li>Check logs: <code>kubectl logs &lt;pod-name&gt;</code>.</li> <li>Attach Prometheus by deploying Prometheus and configuring it to scrape metrics.</li> </ul>"},{"location":"questions/q1/#22-how-would-you-define-a-configmap-and-secrets-in-kubernetes","title":"22. How would you define a ConfigMap and Secrets in Kubernetes?","text":"<ul> <li>ConfigMap: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example-config\n\ndata:\n  key: value\n</code></pre></li> <li>Secrets: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: example-secret\n\ndata:\n  key: c2VjcmV0\n</code></pre></li> </ul>"},{"location":"questions/q1/#23-what-is-the-default-scaling-in-kubernetes-and-how-does-it-work","title":"23. What is the default scaling in Kubernetes, and how does it work?","text":"<ul> <li>Kubernetes uses the Horizontal Pod Autoscaler to scale pods based on metrics like CPU or memory usage.</li> </ul>"},{"location":"questions/q1/#24-what-is-rbac-in-kubernetes-and-why-is-it-important","title":"24. What is RBAC in Kubernetes, and why is it important?","text":"<ul> <li>RBAC (Role-Based Access Control) restricts resource access based on roles.</li> <li>It improves security by enforcing the principle of least privilege.</li> </ul>"},{"location":"questions/q1/#25-whats-the-difference-between-clusterrole-and-role-in-rbac","title":"25. What\u2019s the difference between ClusterRole and Role in RBAC?","text":"<ul> <li>ClusterRole: Grants permissions across the cluster.</li> <li>Role: Grants permissions within a specific namespace.</li> </ul>"},{"location":"questions/terraform/","title":"Terraform","text":"<p>Project structure and file types explained</p> <p>https://spacelift.io/blog/terraform-files</p>"},{"location":"questions/Python/log_tail/","title":"Log_tail","text":""},{"location":"questions/Python/log_tail/#build-a-python-script-for-log-file-monitoring-and-alerting","title":"Build a Python script for log file monitoring and alerting","text":""},{"location":"questions/Python/log_tail/#problem-statement","title":"Problem Statement:","text":"<p>You are tasked with creating a Python script to monitor a log file in real time. The script should:</p> <p>1) Continuously monitor a given log file (e.g., /var/log/application.log).</p> <p>2) Search for any lines containing the word \"ERROR\" (case-insensitive).</p> <p>3) Once an \"ERROR\" line is detected, the script should:  Print the line to the console.  Send an email alert (you can mock the email part for simplicity).</p> <p>4) The script should keep running indefinitely and monitor the log file for new entries.</p> <p></p>"},{"location":"questions/Terraform/Terraform_cheat_sheet/","title":"Terraform_cheat_sheet","text":""},{"location":"questions/Terraform/Terraform_cheat_sheet/#terraform-cheat-sheet","title":"Terraform Cheat sheet","text":""},{"location":"shell-script/Tips/","title":"Tips","text":"<p>shell</p>"},{"location":"shell-script/Tips/#how-to-perform-arithmetic-operation-add-sub-div","title":"how to perform arithmetic operation add, sub, div","text":"<p>(( )) construct permits arithmetic expansion and evaluation. In its simplest form, a=$(( 5 + 3 )) would set a to 5 + 3.</p> <p>[[]] an alternate form of conditional expressions example [[ $i -le 10 ]]  we can use AND -a and OR -o as well.</p>"},{"location":"shell-script/loop_webiste/","title":"Loop_webiste","text":""},{"location":"shell-script/loop_webiste/#for-loop-script","title":"For Loop Script:","text":"<pre><code>for i in {1..35}; do\n   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2&gt;&amp;1` &amp;&amp; echo \"$test OK\" || echo \"Failed\"';\n   echo \"\"\ndone\n</code></pre>"},{"location":"shell-script/roadmap/","title":"Roadmap","text":"<p>standard input \"&gt;\" \"&gt;&gt;\" standard  output standard error</p> <p>replace</p> <p>append</p> <p>0 standard input 1 standard output 2 standard error</p> <p>grandpa</p>"},{"location":"shell-script/shell-script/","title":"Shell script","text":"<p>Got the solution for this issue. Below are the steps to compile and install the net-utils in the CenOS7</p>"},{"location":"shell-script/shell-script/#repost","title":"repost","text":"<p>CentOS7 AMI does not have ec2-net-utils package to configure secondary interface</p> <p>If cloud-init directives are installed you can instance with user data. in cloud-config format aws,User data and cloud-init directives</p> Tab 1shell-to-install packagesTO backup mysql &amp; pmta &amp; header data push to s3 bucket <pre><code>yum groupinstall \"Development Tools\" -y\nyum install epel-release -y\nyum update -y\nyum -y install git make systemd systemd-units rpm-build systemd-networkd cloud-int\ngit clone --branch=1.x --single-branch --depth 1 https://github.com/aws/amazon-ec2-net-utils.git\ncd amazon-ec2-net-utils/\ncurl -LO 'https://github.com/aws/amazon-ec2-net-utils/archive/1.7.3.tar.gz'\nrpmbuild --define \"_sourcedir $PWD\" -bb amazon-ec2-net-utils.spec\nls ~/rpmbuild/RPMS/noarch/\nyum localinstall ls ~/rpmbuild/RPMS/noarch/amazon-ec2-net-utils-1.5-1.el7.noarch.rpm\nyum localinstall /root/rpmbuild/RPMS/noarch/*.rpm\n</code></pre> <p>``` shell bash</p> <pre><code>&lt;&lt; com\nInstalling Aws cli\n\n\nwget \"http://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\"\nunzip awscli-exe-linux-x86_64.zip\nsudo ./aws/install\n\n\n\n# aws configure and auto insert access key and secret key\n\necho -e \"*\\n*\\n\\n\" | aws configure\n\n&lt;&lt; com\n# file will not be created at first installation initilize with \"aws configure\" command\ncat &lt;&lt;EOF &gt;&gt; /root/.aws/credentials\n[default]\naws_access_key_id = *\naws_secret_access_key = *\nEOF\ncom\n\n\n# get server Name From the file | server alias line and get the value | trimer whitespaces\na=$(cat /var/www/cgi-bin/server_config.txt | awk -F 'server alias: ' '{print $2}' | tr -d '[:space:]' )\n\nservername=${a^^} # Smaller To Caps\necho \"$servername\"\n\nsudo mkdir /home/\"$servername\"_old_centos6_backup\n\n#Mysql dump \nmysqldump -u root -p'cubesphpmyadmin'  maildynamix  &gt; /home/\"$servername\"_old_centos6_backup/db_backup.sql\n\n#tar \ntar -cvf /home/\"$servername\"_old_centos6_backup/\"$servername\".pmta_backup.tar /etc/pmta\n\n# using aws cli\naws s3 sync /home/\"$servername\"_old_centos6_backup/ s3://headerv2/header_data/\"$servername\"/\"$servername\"_old_centos6_backup/\n\naws s3 sync /var/www/Maildynamix/Headers/ s3://headerv2/header_data/\"$servername\"/\"$servername\"_old_centos6_backup/Headers\n\n#confirming wether the script is finished or not\n\n\nif [[ \"${?}\" == 0 ]]  # checking  previously executed command is successful or not by using \"$?\"\nthen   \n    echo \"Update Done\"\nelse\n    echo \"Update not done\"\nfi\n</code></pre>"},{"location":"shell-script/shell-script/#binbash","title":"!/bin/bash","text":"<p>packages=(\"httpd\" \"mysql-server\" \"php\")</p>"},{"location":"shell-script/shell-script/#loop-through-packages-and-install","title":"Loop through packages and install","text":"<p>for package in \"${packages[@]}\" do if ! rpm -qa | grep -qw $package; then echo \"Installing $package\" yum install -y $package else echo \"$package is already installed\" fi done</p>"},{"location":"shell-script/shell-script/#check-if-packages-are-installed","title":"Check if packages are installed","text":"<p>for package in \"${packages[@]}\" do if ! rpm -qa | grep -qw $package; then echo \"$package installation failed\" fi done</p> <p>echo \"Package installation complete.\" ```</p>"}]}